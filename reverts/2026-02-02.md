# Week of 2026-02-02 to 2026-02-09 (40)

### Auto Revert (7)

- [Revert "[DTensor] tests for uneven/zero-size shards (#174466)"](https://github.com/pytorch/pytorch/commit/1e1ac0fd876dcd85a38744c372e40f07e91c0ede)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174466#issuecomment-3868226974))
- [Revert "[DTensor] tests for uneven/zero-size shards (#174466)"](https://github.com/pytorch/pytorch/commit/8017d9d03e809164998f104ec43d30cc14a12eac)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174466#issuecomment-3866390102))
- [Revert "[dynamic shapes] fix masked op DDEs (#173402)"](https://github.com/pytorch/pytorch/commit/e1bcc373f778e58753bc37fdb40da86a9857693f)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173402#issuecomment-3863419998))
- [Revert "upgrade lintrunner from 0.2.7 to 0.2.11 (#173658)"](https://github.com/pytorch/pytorch/commit/abe16e11a7ea001370cc08592caa8901163c024b)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173658#issuecomment-3849169332))
- [Revert "use optimizations_hints in defake (#174153)"](https://github.com/pytorch/pytorch/commit/e97896a1ee43135b1580d0ab227f0de4d9bd7d28)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174153#issuecomment-3842993401))
- [Revert "Fixes registration of custom sharding strategies when using register_sharding with TensorList (#172829)"](https://github.com/pytorch/pytorch/commit/280457c54aeecb5d36ed2b09dad16d85e975c986)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/172829#issuecomment-3840152082))
- [Revert "[ONNX] Implement while_loop (#162645)"](https://github.com/pytorch/pytorch/commit/e43586fcd2ac3e6cf144337a2154491235109201)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/162645#issuecomment-3837843434))

### GHFirst (13)

- [Revert "[SymmMem] put_signal and wait_signal (#174034)"](https://github.com/pytorch/pytorch/commit/37753b6117a7447c4137a4afb62d99f16fa4ccc0)
  - crashes internally with:  Type intrusive_ptr<c10d::symmetric_memory::SymmetricMemory> could not be converted to any of the known types.  ([comment](https://github.com/pytorch/pytorch/pull/174034#issuecomment-3865050917))
- [Revert "[DTensor][autograd] Preserve DTensor type for the gradients of unused output (#173295)"](https://github.com/pytorch/pytorch/commit/367fb547dc14cfaca4e9150260309ad8e3cf8cd6)
  - breaks internal signals ([comment](https://github.com/pytorch/pytorch/pull/173295#issuecomment-3865043743))
- [Revert "[DTensor] Handle None gradients in from_local and to_local backward functions (#173865)"](https://github.com/pytorch/pytorch/commit/6bfc279c0840c24f329a97ff4c1d0eb0cab89dbd)
  - breaks internal signals ([comment](https://github.com/pytorch/pytorch/pull/173295#issuecomment-3865043743))
- [Revert "[DTensor] tests for uneven/zero-size shards (#174466)"](https://github.com/pytorch/pytorch/commit/5e4af0eab441f716bcab4c5cf404dae61d1a5138)
  - blocks revert of https://github.com/pytorch/pytorch/pull/173295 ([comment](https://github.com/pytorch/pytorch/pull/174466#issuecomment-3865041143))
- [Revert "Add PDL support to combo kernels (#174232)"](https://github.com/pytorch/pytorch/commit/f18a7faf5297504a3b32edd30e50dc68e0d8f333)
  - Fails triton tests internally, see D92352320 ([comment](https://github.com/pytorch/pytorch/pull/174232#issuecomment-3856925511))
- [Revert "[functh] enable torch._functorch.config.guess_tangent_strides_as_outputs for OSS by default (#173668)"](https://github.com/pytorch/pytorch/commit/25f85afb813626b30ff7c669efcff03beaad8020)
  - breaks internal tests, see D92339525 ([comment](https://github.com/pytorch/pytorch/pull/173668#issuecomment-3855958527))
- [Revert "[DTensor] Optimize redistribute comms using flattened meshes (#172610)"](https://github.com/pytorch/pytorch/commit/783f8998b7d09f8e63aa79da2e56c9388a2e4dbe)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/172610#issuecomment-3844123010))
- [Revert "[DTensor] make debugmode print optimized transforminfos (#173436)"](https://github.com/pytorch/pytorch/commit/dae2a80dfc5a4a57f012e349c609219ae337bffb)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/173436#issuecomment-3844099253))
- [Revert "[dtensor] fix flatten mesh dims arg relative to submesh (#173790)"](https://github.com/pytorch/pytorch/commit/be94add16fdc9f39856b7dcc0c161da40b32e262)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/173790#issuecomment-3843976922))
- [Revert "fix redistribute() handling for finding flattened device mesh dims under compile (#173873)"](https://github.com/pytorch/pytorch/commit/356094eeca55b33566a894c6d31c55f34d6b597d)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/173873#issuecomment-3843932248))
- [Revert "[c10d] Re-enable nonblocking API mode (#173758)"](https://github.com/pytorch/pytorch/commit/a5f71a71b9bb9f275c4c39a17251b00a39355965)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/173758#issuecomment-3843078141))
- [Revert "Make placements opaque (#171482)"](https://github.com/pytorch/pytorch/commit/e5aa7564aefdf500bf7656e83fe80cebc06b1a1e)
  - breaking internal conveyor, see D91862782 ([comment](https://github.com/pytorch/pytorch/pull/171482#issuecomment-3838926299))
- [Revert "Make ProcessGroup opaque (#172566)"](https://github.com/pytorch/pytorch/commit/4eb6bffbe87cd2489a3cfe7d2e8a8da889decbcc)
  - breaking internal conveyor, see D91862782 ([comment](https://github.com/pytorch/pytorch/pull/171482#issuecomment-3838926299))

### Ignored Signal (2)

- [Revert "[ROCm] Updating failing unit tests: Navi & MI200 & MI300 & MI350 (#172780)"](https://github.com/pytorch/pytorch/commit/c6681c3ec06a74797d97b809b24026d4f7cba2c2)
  - breaks test_aot_inductor.py: ([comment](https://github.com/pytorch/pytorch/pull/172780#issuecomment-3849949318))
- [Revert "Beef up cache bypass reason (#173832)"](https://github.com/pytorch/pytorch/commit/2776680aeec80adde3cceb73abe9086e2c691ac4)
  - It broke lint, which was clearly displayed on PR ([comment](https://github.com/pytorch/pytorch/pull/173832#issuecomment-3842771911))

### Not through pytorchbot (1)

- [Revert "[ROCm][CI] skip test_max_autotune until resolved (#162496)" (#162803)](https://github.com/pytorch/pytorch/commit/b6e84619198bb8e1a5268d87837e89e6e55c8fc9)

### No Signal (16)

- [Revert "[CUDA] Fix cuBLAS/cuBLASLt library version mismatch (#174320)"](https://github.com/pytorch/pytorch/commit/ae915b0379d48004ec1f084fb701618c7879f0f6)
  - breaks torchcomms: https://github.com/pytorch/pytorch/issues/174486 ([comment](https://github.com/pytorch/pytorch/pull/174320#issuecomment-3862124622))
- [Revert "properly mark tangent placeholders in the joint graph with "is_backward" (#172868)"](https://github.com/pytorch/pytorch/commit/25c5ce26ac10a28c300fc2a710ed0af3c9cda095)
  - breaks test_stream_backward_sync ([comment](https://github.com/pytorch/pytorch/pull/172868#issuecomment-3857355655))
- [Revert "[aten] Add batch_sizes device check to RNN packed sequence operators (#174192)"](https://github.com/pytorch/pytorch/commit/424da169da564f29793d35505b59ea05f2e021f9)
  - breaks rnn test ([comment](https://github.com/pytorch/pytorch/pull/174192#issuecomment-3849928126))
- [Revert "[FSDP2] enable more tests on CPU (#174048)"](https://github.com/pytorch/pytorch/commit/f24d8327b773d062c3df5a8b2d284806cf0c3fd5)
  - Updated tests seems to timeout on trunk ([comment](https://github.com/pytorch/pytorch/pull/174048#issuecomment-3848452976))
- [Revert "Add PDL support to combo kernels (#174232)"](https://github.com/pytorch/pytorch/commit/5827cabd8b7619cfb29865a22973bf8cd2792dc8)
  - test/inductor/test_combo_kernels.py::ComboKernelPDLTests::test_pdl_combo_kernel_pointwise [GH job link](https://github.com/pytorch/pytorch/actions/runs/21654287926/job/62429790291) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/7cf9f13d676845e2e268e38b270b3e6f4d0ada87) ([comment](https://github.com/pytorch/pytorch/pull/174232#issuecomment-3848428130))
- [Revert "[MAGMA][CUDA] eig: deprecate MAGMA and dispatch to cuSOLVER unconditionally (#173510)"](https://github.com/pytorch/pytorch/commit/c86c762b24bedc18716a27aa8817c0225756795f)
  - See https://github.com/pytorch/pytorch/issues/174281 ([comment](https://github.com/pytorch/pytorch/pull/173510#issuecomment-3847457329))
- [Revert "[MAGMA] cholesky: deprecate MAGMA and dispatch to cuSOLVER unconditionally (#173515)"](https://github.com/pytorch/pytorch/commit/2525b44abbe4dea282f2302df876691261f2c26e)
  - See https://github.com/pytorch/pytorch/issues/174281 ([comment](https://github.com/pytorch/pytorch/pull/173515#issuecomment-3847442101))
- [Revert "[MAGMA] qr: deprecate MAGMA and dispatch to cuSOLVER unconditionally (#173788)"](https://github.com/pytorch/pytorch/commit/2f051fddc6267d2ee1ed164cdcd4da162524548f)
  - See https://github.com/pytorch/pytorch/issues/174281 ([comment](https://github.com/pytorch/pytorch/pull/173788#issuecomment-3847431358))
- [Revert "Update FSDP1 tests to use MultiProcContinuousTest (#173689)"](https://github.com/pytorch/pytorch/commit/1199af79fdea02dde42c021ad5c37c0794dd4dc1)
  - causes deadlocking of some FSDP tests ([comment](https://github.com/pytorch/pytorch/pull/173689#issuecomment-3844266151))
- [Revert "Update FSDP tests to use DTensorContinuousTestBase (#173728)"](https://github.com/pytorch/pytorch/commit/aa2c56ba36f4a0a49cd69a1ef9b0c23c60ebab10)
  - causes deadlocking of some FSDP tests ([comment](https://github.com/pytorch/pytorch/pull/173689#issuecomment-3844266151))
- [Revert "Update PP tests to use MultiProcContinuousTest (#173807)"](https://github.com/pytorch/pytorch/commit/eda046c45b78d183531d14d0e285cc07b67e92cc)
  - causes deadlocking of some FSDP tests ([comment](https://github.com/pytorch/pytorch/pull/173689#issuecomment-3844266151))
- [Revert "[distributed] Remove eager wait() in all_gather_tensor (#172633)"](https://github.com/pytorch/pytorch/commit/63b4ebb4e5c2c07d961d4fd0b511a7701f43a263)
  - TestDTensorDebugMode::test_output_placements ([comment](https://github.com/pytorch/pytorch/pull/172633#issuecomment-3843093480))
- [Revert "[DTensor] Test pointwise partial propagation (#174000)"](https://github.com/pytorch/pytorch/commit/e819e8e553fecd9ddec7352df2bfa4f5079192fa)
  - test/distributed/tensor/test_pointwise_ops.py::PointwisePartialsTest::test_sub_replicate_partial_min_gives_partial_max [GH job link](https://github.com/pytorch/pytorch/actions/runs/21617187245/job/62324663867) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/e2d81fb2c5d8119b55e8e812da9229320928c16d) ([comment](https://github.com/pytorch/pytorch/pull/174000#issuecomment-3842522871))
- [Revert "Add deep nested module instruction count benchmark (#173891)"](https://github.com/pytorch/pytorch/commit/c27a71a0afd8aa3a2a7ff333d708a8f8948b9449)
  - looks like the expected value is different on CI ([comment](https://github.com/pytorch/pytorch/pull/173891#issuecomment-3839009436))
- [Revert "[FSDP2] support dataclass args/kwargs output (#173415)"](https://github.com/pytorch/pytorch/commit/db9959e6afc10e3fdd937cf72b1db0b3b5b1637b)
  - unexpected extra memory usage ([comment](https://github.com/pytorch/pytorch/pull/173415#issuecomment-3833983052))
- [Revert "Fix NaN propagation in pdist (#173826)"](https://github.com/pytorch/pytorch/commit/e0abf7240e22260e0456123af96d56d10ef42499)
  - Sorry, but this broke ROCm CI: https://hud.pytorch.org/failure?name=rocm-mi300%20%2F%20linux-noble-rocm-py3.12-mi300%20%2F%20test%20(default%2C%204%2C%206%2C%20linux.rocm.gpu.gfx942.1)&jobName=undefined&failureCaptures=test%2Ftest_nn.py%3A%3ATestNN%3A%3Atest_pdist_inf_nan_propagation ([comment](https://github.com/pytorch/pytorch/pull/173826#issuecomment-3832721753))

### Weird (1)

- [Revert "[BE] Remove duplicate typing-extension from requirements (#174490)"](https://github.com/pytorch/pytorch/commit/cc7a3cd082cd7c95fec88ba647ac49b2971b30b2)
  - I need it to stay at 4.10 for a bit ([comment](https://github.com/pytorch/pytorch/pull/174490#issuecomment-3863204198))
