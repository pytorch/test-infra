# Week of 2025-12-08 to 2025-12-15 (43)

### Auto Revert (12)

- [Revert "[dynamo] Fix state leakage from test_streams.py (#170333)"](https://github.com/pytorch/pytorch/commit/e9a55c87f6877ba17a5fb03e210ed9d1aca75e70)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170333#issuecomment-3649881747))
- [Revert "[xpu][fix] Fix UT test_fuse_mix_order_reductions_combo_kernels (#170297)"](https://github.com/pytorch/pytorch/commit/cc4899c0561ffdf88c9585ae9f670054d663f863)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170297#issuecomment-3649383415))
- [Revert "Fix unsafe_chunk decomposition for empty tensors (#170053)"](https://github.com/pytorch/pytorch/commit/d75f3668e710d93172f0fdb256f58c7e8d953267)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170053#issuecomment-3648871446))
- [Revert "[Inductor] Async Pipelined Autotune (#170128)"](https://github.com/pytorch/pytorch/commit/47eb1311d3ec7e9d5c2e5b7f6f06125ad146f1c3)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170128#issuecomment-3647986399))
- [Revert "Differentiability Support for Functional Collectives (#168140)"](https://github.com/pytorch/pytorch/commit/bbef332f298b59a71e21e573a4d9f58073b34e20)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/168140#issuecomment-3644321596))
- [Revert "Add python stack and fix js formatting (#170129)"](https://github.com/pytorch/pytorch/commit/ceb1cb00c10f9f69303dda5ef5c856e78fb97620)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170129#issuecomment-3643608019))
- [Revert "[c10d][Sym mem] Make nccl backend full fledged with nccl 2.28.9-1 (#168129)"](https://github.com/pytorch/pytorch/commit/0ddb8edc7c75a35b8718403ec21f67440f464f2c)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/168129#issuecomment-3642750297))
- [Revert "[c10d][Sym mem] Make nccl backend full fledged with nccl 2.28.9-1 (#168129)"](https://github.com/pytorch/pytorch/commit/8121f2c5d0dbbee6322c7fadea90729573b85d4d)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/168129#issuecomment-3641141197))
- [Revert "[FX] Fix node normalization w.r.t. keyword "from" which breaks FX codegen (#169328)"](https://github.com/pytorch/pytorch/commit/340b2abc8562e28382469f25f7994a235d3ca0d2)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169328#issuecomment-3640419814))
- [Revert "ci: Ensure we pass through TPU env variables (#170099)"](https://github.com/pytorch/pytorch/commit/e3102f078ae1c5af4c9ea314e17ab0979a19e4dc)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170099#issuecomment-3639443129))
- [Revert "[inductor][fx] clarify padding logic (#169577)"](https://github.com/pytorch/pytorch/commit/76b8fdafad537579cd97b5ec2502c6ac3bc473e0)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169577#issuecomment-3637208482))
- [Revert "[profiler] Change dynamo_timed to use torch._C._profiler._RecordFunctionFast (#169858)"](https://github.com/pytorch/pytorch/commit/63cf0681cdb00e3ac779b40130b9491b5dc8298e)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169858#issuecomment-3634386782))

### GHFirst (12)

- [Revert "Fix redefining of TOCH_HIP_API (#170149)"](https://github.com/pytorch/pytorch/commit/51f4ff3207554e995541866ede93ec76adfce8de)
  - breaks internal buidls ([comment](https://github.com/pytorch/pytorch/pull/170149#issuecomment-3649728775))
- [Revert "Remove outdated CUDA code (#170273)"](https://github.com/pytorch/pytorch/commit/540dea8732cae93653a99f5995950f624f399af6)
  - Breaks internal builds ([comment](https://github.com/pytorch/pytorch/pull/170273#issuecomment-3649721778))
- [Revert "Fix resource leak: destroy CUDA event after waiting on it (#169790)"](https://github.com/pytorch/pytorch/commit/c30fd3adda6810bbf03d4076d6db2db7718359fd)
  - Reverted internally @albanD please follow up with me on this, cc @izaitsevfb ([comment](https://github.com/pytorch/pytorch/pull/169790#issuecomment-3649654550))
- [Revert "[Inductor] ReLU/GELU(Addmm) fusions (#168157)"](https://github.com/pytorch/pytorch/commit/bdad80fbe3634320dc4a70192da6c3d090c20454)
  - needs to be evaluated for NE regression internally ([comment](https://github.com/pytorch/pytorch/pull/168157#issuecomment-3648542973))
- [Revert "[dynamo] Preserve record function in aot_eager (#167787)"](https://github.com/pytorch/pytorch/commit/8c5e14f76f2250bda3564a4f3eb8bb172b752265)
  - fails internal tests, see D89016549 ([comment](https://github.com/pytorch/pytorch/pull/167787#issuecomment-3648353641))
- [Revert "[ROCm][CI] Change diskspace check to be absolute rather than percentage (#170097)"](https://github.com/pytorch/pytorch/commit/6b553b0f114299fa73d86d7105fc217dc79ddab7)
  - Temporary change to diskspace-cleanup.yml wasn't reverted ([comment](https://github.com/pytorch/pytorch/pull/170097#issuecomment-3647958136))
- [Revert "[FX] Fix node normalization w.r.t. keyword "from" which breaks FX codegen (#169328)"](https://github.com/pytorch/pytorch/commit/7eff3fac7860322a4039c13ea333a5de66f4ae78)
  - oops, this was reverted internally and should stay reverted ([comment](https://github.com/pytorch/pytorch/pull/169328#issuecomment-3644875123))
- [Revert "[inductor][fx] clarify padding logic (#169577)"](https://github.com/pytorch/pytorch/commit/3fd27fe94202b4a2b7c39e2ba99ebfd92e173f84)
  - my bad, this is actually reverted internally ([comment](https://github.com/pytorch/pytorch/pull/169577#issuecomment-3644754204))
- [Revert "[Inductor XPU GEMM] Step 1/N: Refactor cutlass configuration. (#160174)"](https://github.com/pytorch/pytorch/commit/8b21f924c3c24d4cbe239265c87aeb7b271fe974)
  - Sorry for reverting your change, but it seems to cause a perf regression ([comment](https://github.com/pytorch/pytorch/pull/160174#issuecomment-3644336456))
- [Revert "[c10d][Sym mem] Make nccl backend full fledged with nccl 2.28.9-1 (#168129)"](https://github.com/pytorch/pytorch/commit/1d97422133ab0423b648b4ef5537b02aa8761812)
  - fails internal builds, please see D88758774 ([comment](https://github.com/pytorch/pytorch/pull/168129#issuecomment-3635033313))
- [Revert "Avoid std::tie and returning value constructions in qconv_unpack.cpp (#169207)"](https://github.com/pytorch/pytorch/commit/9825967b8e8e38f9552c77aa898c325e3a1ae03d)
  - Sorry for revert this change (again), but there are some new failures showing up and the blast radius is large so I need to revert ([comment](https://github.com/pytorch/pytorch/pull/169207#issuecomment-3635000903))
- [Revert "[Inductor XPU GEMM] Step 2/N: Move out cutlass files from torch/_inductor/codegen/cuda (#160685)"](https://github.com/pytorch/pytorch/commit/a208ed2b54edec0509d4706433d5d2f1f68dcbef)
  - BC-breaking, meta codebase needs to be patched to land this ([comment](https://github.com/pytorch/pytorch/pull/160685#issuecomment-3629881394))

### Ignored Signal (2)

- [Revert "[17/N] Use Python 3.10 typing  (#169735)"](https://github.com/pytorch/pytorch/commit/ef516689bdd9461d7229b5b401d42b8a1d48edbe)
  - causing failures on cuda and rocm ([comment](https://github.com/pytorch/pytorch/pull/169735#issuecomment-3647398666))
- [Revert "[CI] Add eager tests for CUDA 13.0 (#167207)"](https://github.com/pytorch/pytorch/commit/2570a8323cab7687cb9fe7b64837689efb46452c)
  - This breaks CI left and right, see https://hud.pytorch.org/hud/pytorch/pytorch/main/1?per_page=50&name_filter=pull%20%2F%20linux-jammy-cuda13 ([comment](https://github.com/pytorch/pytorch/pull/167207#issuecomment-3629524765))

### Not through pytorchbot (2)

- [Revert PR#161280 "[ROCm][inductor] heuristic improvements for reduction kernels" (#169792)](https://github.com/pytorch/pytorch/commit/297af69667fcd0a4ed4243ea1c9bef75d6b69b09)
- [Revert "Create a mock benchmark results for torchao cudagraphs_low_precision (#123419)" (#169753)](https://github.com/pytorch/pytorch/commit/e1f51479c89dc67dd0aecc1adac8cd9400beedab)

### No Signal (9)

- [Revert "Fix redefining of TOCH_HIP_API (#170149)"](https://github.com/pytorch/pytorch/commit/dd1f0f8f66671424347b323f59beae2964254491)
  - breaks ROCm windows builds, no CI for this yet ([comment](https://github.com/pytorch/pytorch/pull/170149#issuecomment-3652335590))
- [Revert "Change NamedTupleVariable implementation to subclass UserDefinedTupleVariable (#167468)"](https://github.com/pytorch/pytorch/commit/a0f5941246d385a1900a5d3a1296687f389fbed4)
  - Sorry for reverting your change but it seems to break a vLLM test ([comment](https://github.com/pytorch/pytorch/pull/167468#issuecomment-3649864107))
- [Revert "[DTensor] min/max dim reduction fix 2 (#170066)"](https://github.com/pytorch/pytorch/commit/d5c99e5d4ee7b601e8fd6455f9b067f4109747f6)
  - reenabled test is failing on master ([comment](https://github.com/pytorch/pytorch/pull/170066#issuecomment-3644069427))
- [Revert "Add `symm_mem_sync` Triton kernel to `torch.ops.symm_mem` (#168917)"](https://github.com/pytorch/pytorch/commit/b0968f98797f77e5a1b6121491f4e96c0c186d8c)
  - Sorry for reverting the change but I am seeing some distributed vLLM tests failing after this lands ([comment](https://github.com/pytorch/pytorch/pull/168917#issuecomment-3634355405))
- [Revert "[OpenReg] Fixed the issue where streams and events could still be bou… (#169052)"](https://github.com/pytorch/pytorch/commit/fe95af70ad969d5ad88bd1e18c01c3481ef5243b)
  - Sorry for reverting your change but it seems to break some test_streams tests in trunk ([comment](https://github.com/pytorch/pytorch/pull/169052#issuecomment-3631254415))
- [Revert "[BE][Typing][Dynamo] Type torch/_dynamo/variables/user_defined.py (#169319)"](https://github.com/pytorch/pytorch/commit/ee22ae15f8909f41a4cb7a680fb239027eebcf3a)
  - causes benchmark regression ([comment](https://github.com/pytorch/pytorch/pull/169319#issuecomment-3630463955))
- [Revert "[dynamo] Fix frozen dataclass reconstruction (#169614)"](https://github.com/pytorch/pytorch/commit/05db9e354cf984ddfb77d7c238a7d4a0a7a8b2d1)
  - breaks lint ([comment](https://github.com/pytorch/pytorch/pull/169614#issuecomment-3630094053))
- [Revert "Mempool use_on_oom order (#169699)"](https://github.com/pytorch/pytorch/commit/a9dd532d999fd355273d7be30a02d58094c56d05)
  - Failing internal test due to shadow variables. Will reland with fix. ([comment](https://github.com/pytorch/pytorch/pull/169699#issuecomment-3628386257))
- [Revert "Support for vector of Tensors in autograd::Function (#169155)"](https://github.com/pytorch/pytorch/commit/7c593b99eb9bcb1949924a2257ad6a063c08476f)
  - Breaking internal tests ([comment](https://github.com/pytorch/pytorch/pull/169155#issuecomment-3627839884))

### Weird (6)

- [Revert "[c10d][Sym mem] Make nccl backend full fledged with nccl 2.28.9-1 (#168129)"](https://github.com/pytorch/pytorch/commit/fc85d4bd2f305b6871673194192602c620472b7f)
  - the auto-revert was right ([comment](https://github.com/pytorch/pytorch/pull/168129#issuecomment-3643644819))
- [Revert "[ROCm][CI] Build therock with pytorch nightly (#168377)"](https://github.com/pytorch/pytorch/commit/3c551c2480fa4d38f3fe634a39e7033a29a2517e)
  - after a rebase, some non-rocm docker images were removed from the build list, missed during re-review ([comment](https://github.com/pytorch/pytorch/pull/168377#issuecomment-3639211174))
- [Revert "Fix evaluating sympy constant causes error (#169726)"](https://github.com/pytorch/pytorch/commit/28f8916f7933d3210fa671666499f4bc2c3937dc)
  - unexpected success ([comment](https://github.com/pytorch/pytorch/pull/169726#issuecomment-3634102271))
- [Revert "Add torch.backends.cuda.math_sdp.fp32_precision (#169694)"](https://github.com/pytorch/pytorch/commit/c6b3d0f359c59a1390b940d1857dadc3f76653c9)
  - As mentioned in my comment above, please wait for Driss to approve before merging ([comment](https://github.com/pytorch/pytorch/pull/169694#issuecomment-3634065809))
- [Revert "Fixes the sparse tensor issue (#163535)"](https://github.com/pytorch/pytorch/commit/da637782dc8d6f2b759c180d07d641c44fd60b4b)
  - Per @pearu's comment it should not have been merged ([comment](https://github.com/pytorch/pytorch/pull/163535#issuecomment-3632777500))
- [Revert "dynamo torch factory functions apply default device mirroring __torch… (#169081)"](https://github.com/pytorch/pytorch/commit/05c82334cddef4e2c6a5985d38a1a49a9152785c)
  - broken vLLM tests ([comment](https://github.com/pytorch/pytorch/pull/169081#issuecomment-3630100253))
