# Week of 2025-10-20 to 2025-10-27 (32)

### Auto Revert (2)

- [Revert "[Inductor] Naive foreach autotune support (#162053)"](https://github.com/pytorch/pytorch/commit/cf280ca1e82b4c3ce444f853d2f65279ff28b9cf)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/162053#issuecomment-3423808492))
- [Revert "[1/N] Change C-style casts to static_cast or reinterpret_cast (#165750)"](https://github.com/pytorch/pytorch/commit/ab82456c16e8eefcdf0aa960c36d286a2b565431)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165750#issuecomment-3422413890))

### GHFirst (15)

- [Revert "Export should use aot_export_joint_with_descriptors (#165931)"](https://github.com/pytorch/pytorch/commit/690c8c13b966a4952693277d53999cf6a8655f81)
  - breaking internal tests D85084301 for test_auto_functionalize?  I checked that they did run on OSS CI so I'm not entirely sure whats going on, I assume its the IS_FBCODE stuff ([comment](https://github.com/pytorch/pytorch/pull/165931#issuecomment-3443887361))
- [Revert "[DeviceMesh] Implement a device mesh concatenate api for submesh and SPMD use case (#163358)"](https://github.com/pytorch/pytorch/commit/28ee6b62ed7684b36f7659f74260a9284bc45771)
  - probably need to revert this one  too, its stacked with https://github.com/pytorch/pytorch/pull/166003#issuecomment-3443668389 ([comment](https://github.com/pytorch/pytorch/pull/163358#issuecomment-3443874910))
- [Revert "[DeviceMesh] Use _flatten_rank_map to replace _flatten_mesh_list so that we don't need to compare root mesh (#166003)"](https://github.com/pytorch/pytorch/commit/81577bdb3f046159e493bbb44a0fdbf5e68aba1b)
  - failing internal tests D85405179 I believe there are uses of _flatten_mesh_list internally that need to be updated ([comment](https://github.com/pytorch/pytorch/pull/166003#issuecomment-3443668389))
- [Revert "Warn if AccumulateGrad stream does not match producer node stream (#165065)"](https://github.com/pytorch/pytorch/commit/75b8295868525659aa2a31a2302d5676e5bb5263)
  - broke internal builds D85273204 usages of TORCH_API void add need to be updated? ([comment](https://github.com/pytorch/pytorch/pull/165065#issuecomment-3438061854))
- [Revert "[dynamo][misc] Replace UserFunctionVariable with VariableTracker build (#165707)"](https://github.com/pytorch/pytorch/commit/ce8a7764e2f5da76774c5a7618a0daa6d907190b)
  - failing internal tests D85160820 ([comment](https://github.com/pytorch/pytorch/pull/165707#issuecomment-3429084393))
- [Revert "[inductor] require shape in TritonCSEVariable (#162275)"](https://github.com/pytorch/pytorch/commit/240c13394eea0e0b199868163b693bc68efa7b0c)
  - still failing due to the above D84932446 ([comment](https://github.com/pytorch/pytorch/pull/162275#issuecomment-3423153819))
- [Revert "Remove workaround to old CUDA bug (#164354)"](https://github.com/pytorch/pytorch/commit/150682ba7f5081f1b652316d2b822dbc3a0b2da5)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/164354#issuecomment-3423132083))
- [Revert "Move toString(ScalarType) and ScalarType ostream operator to headeronly (#164405)"](https://github.com/pytorch/pytorch/commit/ca7360e9961ef066742ff360620191454e385bb5)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/164354#issuecomment-3423132083))
- [Revert "[dynamo][user_defined] Replace UserFunctionVariable with VariableTracker build (#165706)"](https://github.com/pytorch/pytorch/commit/0bf604320f9eb8d7204014ac4c4b01b1197c51fc)
  - breaking internal tests D84961097 ([comment](https://github.com/pytorch/pytorch/pull/165706#issuecomment-3423059867))
- [Revert "[dynamo][misc] Replace UserFunctionVariable with VariableTracker build (#165707)"](https://github.com/pytorch/pytorch/commit/9875e70da8c31476a3104adf59f104a8d4565f3e)
  - breaking internal tests D84961097 ([comment](https://github.com/pytorch/pytorch/pull/165706#issuecomment-3423059867))
- [Revert "Refactor out headeronly ArrayRef (#164991)"](https://github.com/pytorch/pytorch/commit/69a4bfe8bb1117fd371751b60ded8e9fb1468392)
  - breaking internal tests D84961075 ([comment](https://github.com/pytorch/pytorch/pull/164991#issuecomment-3423058017))
- [Revert "Widen ops support to take in IntHOArrayRef vs only std::vec (#165152)"](https://github.com/pytorch/pytorch/commit/62a263b8d42efdc0d68e81882dbdf8d89229a658)
  - breaking internal tests D84961075 ([comment](https://github.com/pytorch/pytorch/pull/164991#issuecomment-3423058017))
- [Revert "[Submodule] Bump FBGEMM to latest (#165544)"](https://github.com/pytorch/pytorch/commit/0da1f911dcf9e4118dee55610b393d9362de31d3)
  - failing in internal D84996252, probably needs some sort of update to fbgemm internally? ([comment](https://github.com/pytorch/pytorch/pull/165544#issuecomment-3422993703))
- [Revert "[ATen] Fix CUDA reduction warp shuffle order (#164790)"](https://github.com/pytorch/pytorch/commit/602ace5eb4f08ebb9e04ccf13f137160b7d6e8aa)
  - was reverted due to failing internal tests after merge D84992607 ([comment](https://github.com/pytorch/pytorch/pull/164790#issuecomment-3420373755))
- [Revert "12/n : Remove fbandroid_compiler_flags (#165558)"](https://github.com/pytorch/pytorch/commit/47804ce4674ee9cf8f78587092c436469ee253e8)
  - Diff was actually reverted internally D84832629 ([comment](https://github.com/pytorch/pytorch/pull/165558#issuecomment-3420367955))

### Ignored Signal (2)

- [Revert "inductor: avoid unrolling argmin/argmax reductions to preserve index … (#164040)"](https://github.com/pytorch/pytorch/commit/380d440d1c25bad8a708489ace11815db7e614cc)
  - Kindly add the test case mentioned in the issue ([comment](https://github.com/pytorch/pytorch/pull/164040#issuecomment-3444137989))
- [Revert "[ROCm][CI] Update rocm.yml workflow to use 1 GPU ARC runners (#165481)"](https://github.com/pytorch/pytorch/commit/4f7f43253d82b6a61cb2ea40d95cecf06e1ccfa2)
  - Broke lint somehow, see https://hud.pytorch.org/hud/pytorch/pytorch/8f06a1308f256ed7f2610e5e92e06a6871618a06/1?per_page=50&name_filter=lint&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165481#issuecomment-3423642456))

### Not through pytorchbot (3)

- [Reverts #163712 and forces allgather/scatter inputs/outputs to be contiguous (#166181)](https://github.com/pytorch/pytorch/commit/2efcf3ca98e9bac7dc22af310795316457f34d83)
- [Revert "[dynamo][easy] Support torch.accelerator.current_accelerator (#165734)" (#166094)](https://github.com/pytorch/pytorch/commit/2e8e9a59a82394541bb9303bc1bed9d860974712)
- [Back out "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (#164939)" (#165910)](https://github.com/pytorch/pytorch/commit/e6ba4d072510464c846f2013822f9388210eb907)

### No Signal (8)

- [Revert "Export flex attention with kwargs and DTensor (#166045)"](https://github.com/pytorch/pytorch/commit/516e58965aba2a98d633b93924655d5ae1184c22)
  - Broke distributed tests, see https://hud.pytorch.org/hud/pytorch/pytorch/b55b779ad3062b91c64753132264a015378be506/1?per_page=50&name_filter=distributed&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/166045#issuecomment-3446850955))
- [Revert "[inductor][choices] lookup table choices 1/3 (#164978)"](https://github.com/pytorch/pytorch/commit/baf91bbbfc70842f8f99011b88e2626116a459f7)
  - Looks like it broke slow tests, see https://hud.pytorch.org/hud/pytorch/pytorch/cbcb4f776835dfd8350b70a6ffb52503679241b2/1?per_page=50&name_filter=slow&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/164978#issuecomment-3437424559))
- [Revert "[lint] workflow consistency linter to look at all files instead of just changed files (#165171)"](https://github.com/pytorch/pytorch/commit/05b2e02cb4ecbc5544b6c2501f03062c7ca5d51c)
  - broke lint [GH job link](https://github.com/pytorch/pytorch/actions/runs/18723760085/job/53402955955) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/c746feb86a1459db5f6294730d1d72ed15f16dd3) ([comment](https://github.com/pytorch/pytorch/pull/165171#issuecomment-3433501457))
- [Revert "[AMP][Refactor] Autocast dtype handling to simplify device-specific c… (#165221)"](https://github.com/pytorch/pytorch/commit/7773a22cdbe7e3270a998f9d7a4fdba75bb8448e)
  - I think this broke test_openreg [GH job link](https://github.com/pytorch/pytorch/actions/runs/18698271058/job/53322459496) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/4be1e3bf926b8e798fede3be6a3051560e9e00c5) note to self: bad TD ([comment](https://github.com/pytorch/pytorch/pull/165221#issuecomment-3430012693))
- [Revert "[Code Clean] Clean asserts in torch/ao/quantization (root, quantizer, backend_config) (#165433)"](https://github.com/pytorch/pytorch/commit/8daef35cf1867fea5c89c1291fc4cc7b7c6b6f4a)
  - I think this broke some quantization tests ([comment](https://github.com/pytorch/pytorch/pull/165433#issuecomment-3429741770))
- [Revert "shrink_group implementation to expose ncclCommShrink API (#164518)"](https://github.com/pytorch/pytorch/commit/ad4dc52bf6bc09fd3680bcb9bc957203c9cb54f5)
  - Breaks lint ([comment](https://github.com/pytorch/pytorch/pull/164518#issuecomment-3429426503))
- [Revert "[Inductor] support masked vectorization for the tail_loop for float64 datatype (#163316)"](https://github.com/pytorch/pytorch/commit/6c4412f72b6bb017e24623d4c462abb29b7a47b6)
  - seems to have broken some no_gpu tests? test/inductor/test_cpu_repro.py::CPUReproTests::test_double_reduction_vec [GH job link](https://github.com/pytorch/pytorch/actions/runs/18689033019/job/53290772740) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/e9d89734274a4a2640fa77b898c800a87d1d874e) ([comment](https://github.com/pytorch/pytorch/pull/163316#issuecomment-3428210509))
- [Revert "[Inductor] support masked vectorization for the tail_loop for fp8 datatype (#163324)"](https://github.com/pytorch/pytorch/commit/78bf6186f278fd86795064a9dcf205c3e53dddef)
  - seems to have broken some no_gpu tests? test/inductor/test_cpu_repro.py::CPUReproTests::test_double_reduction_vec [GH job link](https://github.com/pytorch/pytorch/actions/runs/18689033019/job/53290772740) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/e9d89734274a4a2640fa77b898c800a87d1d874e) ([comment](https://github.com/pytorch/pytorch/pull/163316#issuecomment-3428210509))

### Weird (2)

- [Revert "[torch/utils][Code Clean] Clean asserts in `torch/utils/*.py` (#165410)"](https://github.com/pytorch/pytorch/commit/bdf7cb9d9cbd51fd17d80ef1e6bab7e5b19bc1c7)
  - sorry I'm going to revert this since I want to try to back out some other things that are conflicting with this, there is nothing wrong with this PR, rebasing and resolving the merge conflicts should be enough, sorry for the churn ([comment](https://github.com/pytorch/pytorch/pull/165410#issuecomment-3427532373))
- [Revert "[ROCm][CI] Update rocm.yml workflow to use 1 GPU ARC runners (#165481)"](https://github.com/pytorch/pytorch/commit/21131a24443ca21aa031b14b1f9e444cfdbe63a7)
  - timeouts after merge ([comment](https://github.com/pytorch/pytorch/pull/165481#issuecomment-3426898171))
