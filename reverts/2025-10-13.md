# Week of 2025-10-13 to 2025-10-20 (40)

### Auto Revert (9)

- [Revert "shrink_group implementation to expose ncclCommShrink API (#164518)"](https://github.com/pytorch/pytorch/commit/633a3b7f67fff48dc1f8ae28c0028930644e3039)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/164518#issuecomment-3419893217))
- [Revert "Enable more DTensor tests in local tensor mode and fix more integration issues (#165716)"](https://github.com/pytorch/pytorch/commit/beb6b62e8c94d7e8683795dda6d3247eb2d30a9b)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165716#issuecomment-3418083391))
- [Revert "Escaped html tags name and target to appear as strings (#165543)"](https://github.com/pytorch/pytorch/commit/06d324365c24395b6d326b2c5e904460bb426dcd)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165543#issuecomment-3417102048))
- [Revert "shrink_group implementation to expose ncclCommShrink API (#164518)"](https://github.com/pytorch/pytorch/commit/fae74cd52f3449ec92fdb519c577c8cd142ab7b1)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/164518#issuecomment-3416718767))
- [Revert "[Mem Snapshot] Add Metadata Field (#165490)"](https://github.com/pytorch/pytorch/commit/11e20843086cf58b3976ed3ac75ac1bbbebd715d)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165490#issuecomment-3413491091))
- [Revert "Add mingw to docker (#165560)"](https://github.com/pytorch/pytorch/commit/69b05913fb0332f9a938c74e26b106e2bd24d82e)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165560#issuecomment-3409814274))
- [Revert "[Inductor][CuTeDSL] Move load_template up two directories (#165347)"](https://github.com/pytorch/pytorch/commit/8c4b528403d68fe3483f0cd3103de44a28409df8)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165347#issuecomment-3407958496))
- [Revert "Patch the flex_attention._get_mod_type to not use inspect.signature when computing num_positional_args (an alternative fix for flex attention graph break on create_block_mask) (#164923)"](https://github.com/pytorch/pytorch/commit/a2f34bdd7ce3a2cf85373854bac75b7cf8069d28)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/164923#issuecomment-3403654378))
- [Revert "Update windows cuda build to use 12.8 (#165345)"](https://github.com/pytorch/pytorch/commit/c5972ebdfb509a0d415fec447d4b7c0df1932fff)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165345#issuecomment-3400344079))

### GHFirst (11)

- [Revert "Enable all PIE rules on ruff (#165814)"](https://github.com/pytorch/pytorch/commit/24520b8386af5f8f95dfe0c1b7d59f506d673bf0)
  - Need to cover more files ([comment](https://github.com/pytorch/pytorch/pull/165814#issuecomment-3417931863))
- [Revert "[Inductor][CuTeDSL] Move load_template up two directories (#165347) (#165576)"](https://github.com/pytorch/pytorch/commit/69c33898fa99f7c4552401a630a77675119c7ce7)
  - This was actually reverted internally, current PR is linked to a stale diff so diff train tools think that this is landed via co-dev when it was actually reverted ([comment](https://github.com/pytorch/pytorch/pull/165576#issuecomment-3417510146))
- [Revert "[DebugMode][2/N] add nn.Module tracking (#165498)"](https://github.com/pytorch/pytorch/commit/b08d8c2e506532ed00c4be5c4a7bfa58c131156d)
  - First part of the stack was reverted so will need to revert this too ([comment](https://github.com/pytorch/pytorch/pull/165498#issuecomment-3416618198))
- [Revert "[DebugMode][1/N] refactor logs into _DebugCalls (#165376)"](https://github.com/pytorch/pytorch/commit/9a71d96256d247109bfb23cdbfce90d8a076115c)
  - This is failing for internal tests, see D84877379 for more context ([comment](https://github.com/pytorch/pytorch/pull/165376#issuecomment-3416570407))
- [Revert "Pyrefly suppressions 2 (#165692)"](https://github.com/pytorch/pytorch/commit/2928c5c5724bec7da91f5a3b24bbd15d5658a0cc)
  - This is causing merge conflicts when attempting to land internally, see D84890919 for more details ([comment](https://github.com/pytorch/pytorch/pull/165692#issuecomment-3416397240))
- [Revert "[ROCm] new implementation of upsample_bilinear2d_backward (#164572)"](https://github.com/pytorch/pytorch/commit/faff826a46c1569eb1c94b0a02299578d1f0e715)
  - Looks like this is failing in our internal builds, will post a suggestion for a fix but want you to double verify that this behavior is correct ([comment](https://github.com/pytorch/pytorch/pull/164572#issuecomment-3416262676))
- [Revert "Fix `_StridedShard` incorrect split (#165533)"](https://github.com/pytorch/pytorch/commit/85c5433d38146dbb30ee410c45fc875ea70b673f)
  - Causing a merge conflict internally, see D84829161 ([comment](https://github.com/pytorch/pytorch/pull/165533#issuecomment-3416143176))
- [Revert "12/n : Remove fbandroid_compiler_flags (#165558)"](https://github.com/pytorch/pytorch/commit/e1d71a6b35318c5d492a3900c84b904be8b8c9de)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/165558#issuecomment-3411879769))
- [Revert "[export] Turn on install_free_tensors flag (#164691)"](https://github.com/pytorch/pytorch/commit/fa3916f4668bf095b1cb8d28bae93554a7ad8bdf)
  - Breaks some internal things, both me and author agreed that revert was the best course of action ([comment](https://github.com/pytorch/pytorch/pull/164691#issuecomment-3400013759))
- [Revert "[opaque_obj_v2] PyObject custom op schema type (#165004)"](https://github.com/pytorch/pytorch/commit/a71ca4dcb914b50b94059ab7bb01fa599b74f399)
  - This fails internal tests, see D84399300 ([comment](https://github.com/pytorch/pytorch/pull/165004#issuecomment-3398906856))
- [Revert "Update round size with 1 division behavior (#162203)"](https://github.com/pytorch/pytorch/commit/955cd7060b7e4ddbdde84e789ae3e22df9f1e7e6)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/162203#issuecomment-3398622898))

### Ignored Signal (1)

- [Revert "[annotate] add annotate_fn function decorator (#165703)"](https://github.com/pytorch/pytorch/commit/80d2ca7566cc38e68b964c1ce168b9320ed8e006)
  - [GH job link](https://github.com/pytorch/pytorch/actions/runs/18585518705/job/52989521797) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/f1d882212afc3a73ce1e319d80b6406f9dc4a0c8) ([comment](https://github.com/pytorch/pytorch/pull/165703#issuecomment-3415073467))

### Not through pytorchbot (1)

- [Revert "[export] Turn on install_free_tensors flag (#164691)" (#165353)](https://github.com/pytorch/pytorch/commit/9166f6120f63e2d5d76e6ccdbfccb8d6e41cbb43)

### No Signal (15)

- [Revert "Update gm.print_readable to include Annotation (#165397)"](https://github.com/pytorch/pytorch/commit/e50dc40d28ba409930023c77a031ec0dd20fd73b)
  - I don't know how/why, but it breaks windows tests, see https://hud.pytorch.org/hud/pytorch/pytorch/2e22b1a61ea20a54448edf34a5d22fbe8391d626/1?per_page=50&name_filter=win&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165397#issuecomment-3417428128))
- [Revert "[export] preserve_node_meta by default (#165524)"](https://github.com/pytorch/pytorch/commit/5d4da26ed067d2d70102f30967f1b09f8fb7018a)
  - test/functorch/test_control_flow.py::TestControlFlowTraced::test_cond_symint_closure [GH job link](https://github.com/pytorch/pytorch/actions/runs/18586312291/job/52991654051) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/fdd560afd1d413a9f814cbf7cc2a72e0d39b0117) ([comment](https://github.com/pytorch/pytorch/pull/165524#issuecomment-3415352522))
- [Revert "Remove torch.serialization entries from the doc ignore list (#160224)"](https://github.com/pytorch/pytorch/commit/574c9fc9503e55f512693eedc52ac627e4330bb6)
  - [GH job link](https://github.com/pytorch/pytorch/actions/runs/18588004962/job/52997748336) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/9fe3b2afbeff12080b483af1ee23e1c9d9fb0421) ([comment](https://github.com/pytorch/pytorch/pull/160224#issuecomment-3415345175))
- [Revert "Turn some const variables into constexpr in C++ code (#165401)"](https://github.com/pytorch/pytorch/commit/9e94ec76b8b29812a1c9dcbb46f00b44e8c3719d)
  - This is breaking test/distributions/test_distributions.py::TestDistributions::test_binomial_sample on HUD, see https://hud.pytorch.org/pytorch/pytorch/commit/5b2afe4c5dc87786ca65bf22ca9a78f7c21a33a4 ([comment](https://github.com/pytorch/pytorch/pull/165401#issuecomment-3414023134))
- [Revert "[Fix] Use sys.executable instead of hardcoded python (#165633)"](https://github.com/pytorch/pytorch/commit/470e2f61c3b2083e8d895b6aae5ede198bba5696)
  - Looks like it broke test_collect_callgrind in slow workflows, see https://hud.pytorch.org/hud/pytorch/pytorch/e0fe37fa687a39e42ddeeb5c03986ffd5c40e662/1?per_page=50&name_filter=slow&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165633#issuecomment-3413290813))
- [Revert "158232  Fix autocast cache incorrectly retaining no_grad state (#165068)"](https://github.com/pytorch/pytorch/commit/d2c82bafb7086a1dd109a0a6407ca7fed27337f4)
  - This broke ROCm CI. test/test_transformers.py::TestTransformersCUDA::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_True_use_autocast_True_d_model_256_cuda [GH job link](https://github.com/pytorch/pytorch/actions/runs/18572589089/job/52952074008) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/5daef30b26b794d237fbbc399c1d47ec0380200a) ([comment](https://github.com/pytorch/pytorch/pull/165068#issuecomment-3413184445))
- [Revert "[inductor] print 0.0 as 0 for triton (#164291)"](https://github.com/pytorch/pytorch/commit/fb06e49ce86c120cb070b0b28c7bd49785a68e43)
  - Broke slow job, see https://hud.pytorch.org/hud/pytorch/pytorch/aba8c43594a83772281a62a7961c0b6ddcff321d/1?per_page=50&name_filter=slow&mergeEphemeralLF=true  ([comment](https://github.com/pytorch/pytorch/pull/164291#issuecomment-3412768915))
- [Revert "[DeviceMesh] Prefer using _layout over _mesh for all sorts of things (#165554)"](https://github.com/pytorch/pytorch/commit/27a98e6ae97a0f82c2deba225b1142b73be2e639)
  - Looks like it broke serialization test, see https://hud.pytorch.org/hud/pytorch/pytorch/aba8c43594a83772281a62a7961c0b6ddcff321d/1?per_page=50&name_filter=distributed%2C%201&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165554#issuecomment-3412765681))
- [Revert "[DeviceMesh] Introduce private constructor instead of _create_mesh_from_ranks (#165555)"](https://github.com/pytorch/pytorch/commit/b10f463b1afaf36f6ae76a0bba476342829a36a0)
  - Looks like it broke serialization test, see https://hud.pytorch.org/hud/pytorch/pytorch/aba8c43594a83772281a62a7961c0b6ddcff321d/1?per_page=50&name_filter=distributed%2C%201&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165554#issuecomment-3412765681))
- [Revert "[DeviceMesh] Simplify unflatten method (#165556)"](https://github.com/pytorch/pytorch/commit/431c13cf617277fd05ddc5d1fc58a3275109f60a)
  - Looks like it broke serialization test, see https://hud.pytorch.org/hud/pytorch/pytorch/aba8c43594a83772281a62a7961c0b6ddcff321d/1?per_page=50&name_filter=distributed%2C%201&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165554#issuecomment-3412765681))
- [Revert "[inductor] Expand use of generic benchmark function (#164938)"](https://github.com/pytorch/pytorch/commit/84d141e910c0e7e86584e2a4625353e333bec2e5)
  - I think this broke test/inductor/test_cuda_repro.py::CudaReproTests::test_epilogue_fusion_with_view? [GH job link](https://github.com/pytorch/pytorch/actions/runs/18529735968/job/52813191763) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/f58f301313d4fc89499fb35cdfb2ffb91d14d896) on both rocm and the slow grad check for linux. It did run successfully on cuda workflow on trunk, I wonder if this a gpu capability thing? no clue though ([comment](https://github.com/pytorch/pytorch/pull/164938#issuecomment-3407600224))
- [Revert "add and fix OpInfo tests for the default partitioner (#165372)"](https://github.com/pytorch/pytorch/commit/b509fb9b5d82575f1126baf3c146dee4db51b581)
  - Looks like it broke slow jobs, see https://hud.pytorch.org/hud/pytorch/pytorch/331b7cc054415210ec73f4e7e4571f8a0c21ed62/1?per_page=50&name_filter=slow&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165372#issuecomment-3407567748))
- [Revert "[distributed] Replace assert statements with AssertionError exceptions (#165216)"](https://github.com/pytorch/pytorch/commit/d2494cbb2b98a3105f0fc3eea79abe7d58df61c6)
  - I think this broke distributed/test_pg_wrapper.py::ProcessGroupNCCLWrapperTest::test_debug_level_detail_no_gloo [GH job link](https://github.com/pytorch/pytorch/actions/runs/18492765290/job/52693842750) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/74db92b21868b7e9e77cc966e5d57a8246723cbd), note to self: bad TD ([comment](https://github.com/pytorch/pytorch/pull/165216#issuecomment-3402838765))
- [Revert "use sym_numel, to allow fake tensors to work (#163831)"](https://github.com/pytorch/pytorch/commit/33bfec27ff867cf5e719fa997f00c1ec3dbb9859)
  - test failure on mps introduced ([comment](https://github.com/pytorch/pytorch/pull/163831#issuecomment-3400131730))
- [Revert "[dynamo][DebugMode] mask python keys in dispatch_key_set guard checks (#164992)"](https://github.com/pytorch/pytorch/commit/85801126821d4f509f3cf5aafa24dbcd3cd11183)
  - broke ROCm CI test/inductor/test_inductor_scheduler.py::TestSchedulerCUDA::test_flop_counter_op_options0_cuda_float32 [GH job link](https://github.com/pytorch/pytorch/actions/runs/18417066364/job/52485636942) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/306b344a1847749f0baf085dcd92560f4e99cd1b) ([comment](https://github.com/pytorch/pytorch/pull/164992#issuecomment-3397927142))

### Weird (3)

- [Revert "varlen api (#164502)"](https://github.com/pytorch/pytorch/commit/3044e1a460a2ae71a95e77d9ac0c33d3e8294e85)
  - Sorry for reverting your change, but the doctests failure is legit ([comment](https://github.com/pytorch/pytorch/pull/164502#issuecomment-3404419420))
- [Revert "[export] Handle kwargs better in aot_export_joint_with_descriptors (#165334)"](https://github.com/pytorch/pytorch/commit/7778a58e7c3a9dfca8c4fa00d936581e7549d918)
  - trunk CI passed here but failures on HUD after merge?  test/functorch/test_aot_joint_with_descriptors.py::TestAOTJointWithDescriptors::test_module_with_kwargs [GH job link](https://github.com/pytorch/pytorch/actions/runs/18511729262/job/52755708742) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/bbb902c8dd911e1587253f496c1e2fb178d4b6a1) ([comment](https://github.com/pytorch/pytorch/pull/165334#issuecomment-3404071893))
- [Revert "Fix double dispatch to Python for detach (#163671)"](https://github.com/pytorch/pytorch/commit/267348fe7fda1ac8aa6b57cbcbe8db0ce6362baa)
  - We should've reverted this when we decided to revert https://github.com/pytorch/pytorch/pull/164691 since they were actually stacked ([comment](https://github.com/pytorch/pytorch/pull/163671#issuecomment-3400009953))
