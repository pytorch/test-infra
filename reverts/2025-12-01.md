# Week of 2025-12-01 to 2025-12-08 (44)

### Auto Revert (3)

- [Revert "Fix torch.fx for the newer "|" union syntax (#169453)"](https://github.com/pytorch/pytorch/commit/b1decff555cd50e2123c8c6e25cc0d447c411f62)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169453#issuecomment-3610098687))
- [Revert "Support module.to in strict export (#167555)"](https://github.com/pytorch/pytorch/commit/e115f9f4e4b039f8e9a642aaa2bd8254a920541b)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/167555#issuecomment-3609697942))
- [Revert "[Accelerator] Add Accelerator Capabilities API (#165631)"](https://github.com/pytorch/pytorch/commit/7d2a33e4ebf60b217a3cd77feae19231eb996fc8)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165631#issuecomment-3595505779))

### GHFirst (16)

- [Revert "[3/N] Remove unused header inclusion (#169200)"](https://github.com/pytorch/pytorch/commit/7dcfb47aa2714c895bc7bcd3c30b4c61983ba8a8)
  - Sorry for reverting your change but it has a small issue in tensorexpr_init.cpp ([comment](https://github.com/pytorch/pytorch/pull/169200#issuecomment-3619486721))
- [Revert "[Dynamo][Guard]Add the user-friendly TYPE_MATCH for type (#169025)"](https://github.com/pytorch/pytorch/commit/4bebe21b056adc8d965b095ede2f4a5fbb923cf8)
  - Abandoning D88461499 seems to lead to a discrepancy between github and the import diff.  I need to revert and reland this to reimport the change.  If your change only touch GitHub OSS, you dont need to do anything with the import diff on diff train and can just reland it ([comment](https://github.com/pytorch/pytorch/pull/169025#issuecomment-3619104268))
- [Revert "[effects] Various effect/dce fixes (#169141)"](https://github.com/pytorch/pytorch/commit/0022a14bf068876d866a7d6fe0e134d6ef5ae771)
  - Chatting with Angela, and revert and reland this as it fails an internal test ([comment](https://github.com/pytorch/pytorch/pull/169141#issuecomment-3618640653))
- [Revert "activation offloading implementation (#167880)"](https://github.com/pytorch/pytorch/commit/fc8c66f48f32feb039d00f6d85c71de4c7b79603)
  - internal CI failure with lazy init on non GPU machines ([comment](https://github.com/pytorch/pytorch/pull/167880#issuecomment-3617457510))
- [Revert "activation offloading reordering for comp<>comm overlaps (#168316)"](https://github.com/pytorch/pytorch/commit/95a2036933173c2f470c3edc21a94cc244298212)
  - internal CI failure with lazy init on non GPU machines ([comment](https://github.com/pytorch/pytorch/pull/168316#issuecomment-3617440610))
- [Revert "[Inductor] ReLU/GELU(Addmm) fusions (#168157)"](https://github.com/pytorch/pytorch/commit/2169e666d2424a84c266b694c9f97a42f786e30e)
  - Sorry for reverting your change but I think it fails test_linear_relu ([comment](https://github.com/pytorch/pytorch/pull/168157#issuecomment-3615072958))
- [Revert "[ROCm] Enable StaticCudaLauncher for ROCm (#166492)"](https://github.com/pytorch/pytorch/commit/6f4baa5abe393cba7792edca4d3b418a630e1ba2)
  - Sorry for reverthing this but the change has been reverted by an internal meta team ([comment](https://github.com/pytorch/pytorch/pull/166492#issuecomment-3614162658))
- [Revert "[export] Make RNNs exportable on GPUs (#163245)"](https://github.com/pytorch/pytorch/commit/ae3a2395bf66151078e2d201716f7d63ce1c6f3e)
  - Reverted internally ([comment](https://github.com/pytorch/pytorch/pull/163245#issuecomment-3610661311))
- [Revert "[Dynamo][Guard]Add the user-friendly TYPE_MATCH for type (#169025)"](https://github.com/pytorch/pytorch/commit/b6b6d912df0b6f4082f8e50b18bd1de1dd7325f4)
  - Sorry for reverting your change but the new tests are failing internally D88329912 ([comment](https://github.com/pytorch/pytorch/pull/169025#issuecomment-3610438983))
- [Revert "[dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker (#169204)"](https://github.com/pytorch/pytorch/commit/ba1412546f3082c0958c077acc2025e4dbc33f1f)
  - This has been reverted internally ([comment](https://github.com/pytorch/pytorch/pull/169204#issuecomment-3610355613))
- [Revert "Avoid std::tie and returning value constructions in qconv_unpack.cpp (#169207)"](https://github.com/pytorch/pytorch/commit/6f53fefeb90ad3281119b5cfc4aa9ffd8a066e3d)
  - Sorry to keep reverting this, but the issue is still there ([comment](https://github.com/pytorch/pytorch/pull/169207#issuecomment-3609906136))
- [Revert "[generator] Close all open generators in compile_subgraph (#157149)"](https://github.com/pytorch/pytorch/commit/2ac3ef882afb23136adc188975f0a8802fc68adf)
  - Sorry for reverting your change but it seems to break some torchrec tests ([comment](https://github.com/pytorch/pytorch/pull/157149#issuecomment-3609616560))
- [Revert " [10/N] Use Python 3.10 typing  (#169229)"](https://github.com/pytorch/pytorch/commit/3418bd29475dff06695045fcdf93e7d0dac67da8)
  - Sorry for reverting your change but there is a subtlety w.r.t export/import that is surfaced by this change ([comment](https://github.com/pytorch/pytorch/pull/169229#issuecomment-3605071398))
- [Revert "[Accelerator] Add Accelerator Capabilities API (#165631)"](https://github.com/pytorch/pytorch/commit/6c261c6cb07892c90ca19ed51c9705b1659a3f7d)
  - Sorry for reverting your change but it has a small bug when building this internally ([comment](https://github.com/pytorch/pytorch/pull/165631#issuecomment-3604616720))
- [Revert "Remove unnecessary uses of thrust::tuple (#168936)"](https://github.com/pytorch/pytorch/commit/70076464a63ab218a7ceefb0e76ccd7131deb8f8)
  - Sorry for reverting your change but we failed to land this due to the mismatch of ROCm version on OSS and internal ([comment](https://github.com/pytorch/pytorch/pull/168936#issuecomment-3603608569))
- [Revert "Avoid std::tie and returning value constructions in qconv_unpack.cpp (#169207)"](https://github.com/pytorch/pytorch/commit/641cdb68ae27668eb441d0e49c87a0602c120c2b)
  - I think this breaks some quantization tests, maybe they were wrongly skipped on CI ([comment](https://github.com/pytorch/pytorch/pull/169207#issuecomment-3599494109))

### Ignored Signal (3)

- [Revert "[precompile] disable dispatch when deepcloning in PrecompileContext.record_artifact (#169242)"](https://github.com/pytorch/pytorch/commit/965c7e6ece25250bb35c4b7b385d5db556cf4325)
  - It really breaks torch.compile on MacOS ([comment](https://github.com/pytorch/pytorch/pull/169242#issuecomment-3617325042))
- [Revert "Add public documentation for stable_topological_sort (#169498)"](https://github.com/pytorch/pytorch/commit/b3a7edb2311367974cc7cd764cfb11a5d6758b24)
  - The doc test failure is legit ([comment](https://github.com/pytorch/pytorch/pull/169498#issuecomment-3609569000))
- [Revert "Enable custom collective op autotuning (#167294)"](https://github.com/pytorch/pytorch/commit/305168768a95d69c444df5cd334bb774edfe06f1)
  - I was too hasty, the macos failure was legit ([comment](https://github.com/pytorch/pytorch/pull/167294#issuecomment-3609375254))

### Not through pytorchbot (1)

- [Revert getAllOperatorsFor changes (#167860, #162218) (#169281)](https://github.com/pytorch/pytorch/commit/8c13e8bc03cbc7807eef8fe72b3bc94bfa04e0c8)

### No Signal (14)

- [Revert "[dynamo] Rehaul the autograd.Function support (#166788)"](https://github.com/pytorch/pytorch/commit/fbb5bb83d4e6e5763bc0c9e76ad0c392cab61d8e)
  - Sorry for reverting your change but it seems to cause some numerical error in trunk ([comment](https://github.com/pytorch/pytorch/pull/166788#issuecomment-3623054283))
- [Revert "Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED (#169044)"](https://github.com/pytorch/pytorch/commit/066e4536968c37b1dca8815e76d1bba235b4e5fb)
  - it still breaks backward_compatibility test ([comment](https://github.com/pytorch/pytorch/pull/169044#issuecomment-3619654524))
- [Revert "Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED (#169044)"](https://github.com/pytorch/pytorch/commit/6bb8cad00f6a955a28b60c493981604df64d89e6)
  - it breaks backward_compatibility test ([comment](https://github.com/pytorch/pytorch/pull/169044#issuecomment-3619388784))
- [Revert "Fix unnecessary super method delegation detection on pylint (#169350)"](https://github.com/pytorch/pytorch/commit/e28e4ec14ea3cecfe7ada7a6afbc07b08a038ea0)
  - Sorry for reverting your change but it seems to break backward_compatibility test ([comment](https://github.com/pytorch/pytorch/pull/169350#issuecomment-3619376783))
- [Revert "[CI] Workaround `relocation truncated to fit: R_AARCH64_CALL26` linker error (#169723)"](https://github.com/pytorch/pytorch/commit/a25bbe5bae03f065a730e82145e4a696df14162d)
  - I've made a typo ([comment](https://github.com/pytorch/pytorch/pull/169723#issuecomment-3619097831))
- [Revert "[BE] Rewrite IndexKernel using Dispatch_v2 (#169673)"](https://github.com/pytorch/pytorch/commit/1e36d310b15842ef8e1618bdbc9d08650c9856a3)
  - This seems to cause a 3.10 dynamo test to fail in trunk ([comment](https://github.com/pytorch/pytorch/pull/169673#issuecomment-3618979437))
- [Revert "[BE] Delete `install_vision` from Docker builds (#169609)"](https://github.com/pytorch/pytorch/commit/09f18009ea088c383418ee63649a73e197997424)
  - It causes inductor tests to fail ([comment](https://github.com/pytorch/pytorch/pull/169609#issuecomment-3615543740))
- [Revert "[dynamo] Refactor isinstance(x, ConstantVariable) to x.is_python_constant() (#169006)"](https://github.com/pytorch/pytorch/commit/6be8f42812e0c472ba5070da925c38764be1577d)
  - Sorry for reverting your change but it seems to break 3.10 dynamo tests ([comment](https://github.com/pytorch/pytorch/pull/169006#issuecomment-3614888197))
- [Revert "Re-enable torch.compile tests for Python 3.12 and Windows (#169387)"](https://github.com/pytorch/pytorch/commit/77b90b703e0e56d2e70c5dc58fe8d967281c5705)
  - needs additional work ([comment](https://github.com/pytorch/pytorch/pull/169387#issuecomment-3614647730))
- [Revert "[effect] Remove special handling for profiler op (#168389)"](https://github.com/pytorch/pytorch/commit/65c4620d6bb0c6029f69762c22b91dda2294da9a)
  - Sorry for reverting your change but it seems to fail test_python_dispatch ([comment](https://github.com/pytorch/pytorch/pull/168389#issuecomment-3609380860))
- [Revert "Triton 3.6 pin update (#168096)"](https://github.com/pytorch/pytorch/commit/fdf863d5e1de3b2688c9511e96876e34581dbfd7)
  - Causes timeouts https://github.com/pytorch/pytorch/issues/169492 ([comment](https://github.com/pytorch/pytorch/pull/168096#issuecomment-3609092057))
- [Revert "[MPS] Fix dlpack exports/imports for sliced tensors (#169272)"](https://github.com/pytorch/pytorch/commit/0c281dd78773b2bc17c58ead0e4cd4ac46e775c5)
  - I am seeing some ROCm failures in trunk after this lands ([comment](https://github.com/pytorch/pytorch/pull/169272#issuecomment-3604521392))
- [Revert "Refactor: Remove unnecessary ConstantVariable wrapping in raise_observed_exception (#168337)"](https://github.com/pytorch/pytorch/commit/1e34fb2550e4aa650314f7a6d9f6daf4da7478a8)
  - Sorry for reverting your change but it seems to fail some dynamo tests in trunk ([comment](https://github.com/pytorch/pytorch/pull/168337#issuecomment-3600738678))
- [Revert "[dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker (#169204)"](https://github.com/pytorch/pytorch/commit/55c4ab554845481d0a69a3811937575fe8bb1a66)
  - failing test/test_fx.py::TestFXAPIBackwardCompatibility::test_public_api_surface [GH job link](https://github.com/pytorch/pytorch/actions/runs/19803784913/job/56735267443) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/84149583d483e9c973c9a0feda70e4f3964947b0) consistently ([comment](https://github.com/pytorch/pytorch/pull/169204#issuecomment-3594928195))

### Weird (7)

- [Revert "[dynamo][hops] Allow side effects in autograd.Function forward graph tracing (#169399)"](https://github.com/pytorch/pytorch/commit/304c0ca15e60e2e210be1b96b6b3a86667120b43)
  - Sorry for reverting your change but I need to revert this to revert https://github.com/pytorch/pytorch/pull/166788 ([comment](https://github.com/pytorch/pytorch/pull/169399#issuecomment-3623047872))
- [Revert "Fix strides for fa4 integration (#169714)"](https://github.com/pytorch/pytorch/commit/67a371749deade1126609c3604b8baf3f57c0694)
  - this is wrong ([comment](https://github.com/pytorch/pytorch/pull/169714#issuecomment-3621434262))
- [Revert "Add Pylint checks to linterrunner (#167421)"](https://github.com/pytorch/pytorch/commit/16696182f6066ee94f9633f4eaa2b710bf0dbc76)
  - PyLint  has become the dominate lintting time ([comment](https://github.com/pytorch/pytorch/pull/167421#issuecomment-3617746014))
- [Revert "Remove unnecessary uses of thrust::tuple (#168936)"](https://github.com/pytorch/pytorch/commit/f575ecb83c0fe1ebad53f39c78c85e2701a1332b)
  - It'll break internal ROCM builds again ([comment](https://github.com/pytorch/pytorch/pull/168936#issuecomment-3614214438))
- [Revert "[opaque obj] Improve error msg for intermediate opaques (#167742)"](https://github.com/pytorch/pytorch/commit/b6b6c80379388b7f9932c3e6a0f9907bf430e417)
  - Sorry, I need to revert this to cleanly revert https://github.com/pytorch/pytorch/pull/169204.  Please rebase and reland this ([comment](https://github.com/pytorch/pytorch/pull/167742#issuecomment-3610340589))
- [Revert "Replace `msg` by `args` in `raise_observed_exception` (#169343)"](https://github.com/pytorch/pytorch/commit/cc0853af42122f8185321f542616f4474e717f09)
  - It looks like there is a land race here failing lint ([comment](https://github.com/pytorch/pytorch/pull/169343#issuecomment-3609956743))
- [Revert "[Dynamo][Guards]Fix TLParse CPP guard message with sorting get_leaf_guards and verbose_code_parts (#169102)"](https://github.com/pytorch/pytorch/commit/c178ed43d3d99cbefe84fbfb21d6f282b20d62ac)
  - Sorry for reverting your change but I need to revert this in order to revert https://github.com/pytorch/pytorch/pull/169229 ([comment](https://github.com/pytorch/pytorch/pull/169102#issuecomment-3605053944))
