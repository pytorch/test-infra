# Week of 2026-01-05 to 2026-01-12 (28)

### Auto Revert (13)

- [Revert "Automatically wrap autograd.grad with allow_in_graph (#169493)"](https://github.com/pytorch/pytorch/commit/584cebd7583b10dcce419fc0101d96f400cb3cb8)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169493#issuecomment-3730831414))
- [Revert "Support tensor.backward in dynamo (#169415)"](https://github.com/pytorch/pytorch/commit/96b6b2fe99eda82d25182308531fd467615cc11c)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169493#issuecomment-3730831414))
- [Revert "[export] Use bytecode to flatten graph inputs. (#171110)"](https://github.com/pytorch/pytorch/commit/4d3d6d74139827880b7ee9e329b4ec1fd660c918)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/171110#issuecomment-3730358510))
- [Revert "Bump aiohttp from 3.13.2 to 3.13.3 in /.ci/docker (#171760)"](https://github.com/pytorch/pytorch/commit/6eba40b48d278cd83c73a6cd6ba88b9c284b762d)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/171760#issuecomment-3727413339))
- [Revert "Automatically wrap autograd.grad with allow_in_graph (#169493)"](https://github.com/pytorch/pytorch/commit/f6c87f57afe9f09cb711b648e879b2252725eb26)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169493#issuecomment-3726649503))
- [Revert "Support tensor.backward in dynamo (#169415)"](https://github.com/pytorch/pytorch/commit/e3d7567206c3ffb97752c0ba831e583804a7086b)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169493#issuecomment-3726649503))
- [Revert "[inductor][caching] add memoizers and encoders for all tuned kernels (#171939)"](https://github.com/pytorch/pytorch/commit/b33d536ee281040d0810be544e06b0c3d2aed84a)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/171939#issuecomment-3723453490))
- [Revert "Fix FileBaton leaving dangling lock files that cause compilation hangâ€¦ (#170679)"](https://github.com/pytorch/pytorch/commit/01515dc34bfa7131f1a33f7400c58e7e1072c1ad)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170679#issuecomment-3720450880))
- [Revert "[cuDNN][Convolution] Disable a cuDNN Convolution engine preemptively (#171747)"](https://github.com/pytorch/pytorch/commit/25317e93954d33f69e5addd018edbf2906c27b2b)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/171747#issuecomment-3718064279))
- [Revert "[precompile] Support external data for serialization. (#170846)"](https://github.com/pytorch/pytorch/commit/904f381a92ce1d43fd657857484399a7a4405715)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170846#issuecomment-3717351143))
- [Revert "[BE][Dynamo][Type Coverage] Turn on strict typing for dynamo with Pyrefly (#171487)"](https://github.com/pytorch/pytorch/commit/3218133c938ddf99ec4d5816640ef91a8e2bfb88)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/171487#issuecomment-3713304838))
- [Revert "Fixes for s390x CI (#169829)"](https://github.com/pytorch/pytorch/commit/12cd037d66853a2ab201709f28f14aa67a807567)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169829#issuecomment-3711745140))
- [Revert "Better error handling in torch/csrc/jit/passes by replacing std::runtime_error with TORCH_CHECK in passes [part 2] (#165736)"](https://github.com/pytorch/pytorch/commit/1b45e25063a035036ee6c2546a9141a8621248f9)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165736#issuecomment-3711238886))

### GHFirst (8)

- [Revert "[cuDNN] Fix cuDNN runtime version parsing for prints (#169139)"](https://github.com/pytorch/pytorch/commit/e45c0c75ac490f13c21a100136e9d3bdb9161a5a)
  - We at Meta still depend on cudnn_frontend versions 1.0.3 and 1.12.1, so we need to maintain backwards compatibility for now ([comment](https://github.com/pytorch/pytorch/pull/169139#issuecomment-3730225447))
- [Revert "Don't treat `torch.use_deterministic_algorithms` as a context manager (#171530)"](https://github.com/pytorch/pytorch/commit/a09659fe9b2dd40d5fb8911f384af6005ac775df)
  - seems to be breaking internal signals, @Lucaskabela please help the author, D90333108 ([comment](https://github.com/pytorch/pytorch/pull/171530#issuecomment-3729070980))
- [Revert "[inductor] Remove implicit float64 upcast in triton (#170393)"](https://github.com/pytorch/pytorch/commit/baa05ac1b98930dc3c342087e6e4553f47b237f1)
  - sorry, need to revert in order to revert https://github.com/pytorch/pytorch/pull/171530, feel free to fix conflicts and merge back in ([comment](https://github.com/pytorch/pytorch/pull/170393#issuecomment-3729055588))
- [Revert "[Inductor] Add fma lowerings for addcmul ops (#170531)"](https://github.com/pytorch/pytorch/commit/dfb132b02f685ed49c4c5ecf3fa46399060c4e93)
  - sorry, need to revert in order to revert https://github.com/pytorch/pytorch/pull/171530, feel free to fix conflicts and merge back in ([comment](https://github.com/pytorch/pytorch/pull/170531#issuecomment-3729002335))
- [Revert "fix guard ordering on nn modules (#170493)"](https://github.com/pytorch/pytorch/commit/d849d6732552ab530f200606e5c64e1c31d2ba6d)
  - seems to be breaking internal signals, see D90273254 ([comment](https://github.com/pytorch/pytorch/pull/170493#issuecomment-3728951430))
- [Revert "[BE][Ez]: Mark aot_autograd descriptors dataclasses slots=True (#171815)"](https://github.com/pytorch/pytorch/commit/9ce7d05f88715bdb02963551e41be017c8a68a02)
  - Seems to be introducing some segfault on internal tests, I suspect the regression is real, @mlazos may you help the author?, see D90223644 ([comment](https://github.com/pytorch/pytorch/pull/171815#issuecomment-3721507058))
- [Revert "Partitioner Hook for Custom Runtime Estimation (#170739)"](https://github.com/pytorch/pytorch/commit/0aaea43fa5c94c78a6274d53f7c5343e919dfc18)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/170739#issuecomment-3717458825))
- [Revert "[CUDA][Green Context] Expose green context streams (#171116)"](https://github.com/pytorch/pytorch/commit/bfde4f9a7e1d97f6022216246e171f1ccc496405)
  - breaks internal builds, see D90148243 ([comment](https://github.com/pytorch/pytorch/pull/171116#issuecomment-3716777938))

### Ignored Signal (1)

- [Revert "[inductor][caching] add memoizers and encoders for all tuned kernels (#171939)"](https://github.com/pytorch/pytorch/commit/d2a682fa61fc66ff1254ebc006cd7a88078b69e1)
  - This is breaking tests on trunk. hud.pytorch.org/ ([comment](https://github.com/pytorch/pytorch/pull/171939#issuecomment-3727433067))

### Not through pytorchbot (1)

- [Revert PR #169782 (commit `91cd44f`) (#171884)](https://github.com/pytorch/pytorch/commit/99171aeafa876e9cf3d6406ef2658d025a7e9c4e)

### No Signal (4)

- [Revert "[BE][CI] Remove `CONDA_RUN`/`print_cmake_info` (#172051)"](https://github.com/pytorch/pytorch/commit/50b638444e3ddd95886f015f819497b91a9de4c7)
  - seems to have introduced regressions on mac signals ([comment](https://github.com/pytorch/pytorch/pull/172051#issuecomment-3728984074))
- [Revert "[decomp] Add max_pool2d_with_indices_backward decomposition using gather+reduction (#167318)"](https://github.com/pytorch/pytorch/commit/e3240d2e80b96386fd4a51c2ed0cf313cf506584)
  - This is breaking tests on trunk. hud.pytorch.org/ ([comment](https://github.com/pytorch/pytorch/pull/167318#issuecomment-3724602256))
- [Revert "[ROCm] Fix fake_quantize undefined behavior with inf (#171777)"](https://github.com/pytorch/pytorch/commit/bebd70e13f6cb34e1d1377b469e89e053c8fb366)
  - Breaks mask calculation for backward pass ([comment](https://github.com/pytorch/pytorch/pull/171777#issuecomment-3721526971))
- [Revert "[Dynamo][Guards]Complete user stack recompilation reason post-processing (#170841)"](https://github.com/pytorch/pytorch/commit/98dbbd562c8e2535df6477231c183295eb550368)
  - Sorry for reverting your change but it seems to break some dynamo tests in trunk ([comment](https://github.com/pytorch/pytorch/pull/170841#issuecomment-3718404228))

### Weird (1)

- [Revert "[CI] Add IoU-based accuracy checking for inductor tests segmentation models (#171927)"](https://github.com/pytorch/pytorch/commit/601fad192b69d1ea9923536ae51d88bfefa7eeef)
  - seems that the defined threshold is still not sufficient, so I'll revert and try to reproduce it to figure out what is the best next steps ([comment](https://github.com/pytorch/pytorch/pull/171927#issuecomment-3729982839))
