# Week of 2026-02-09 to 2026-02-16 (49)

### Auto Revert (24)

- [Revert "[DTensor] support Partial input for matmul in single-dim registration (#174926)"](https://github.com/pytorch/pytorch/commit/7f8196386c750b49e8db55ea5392e727410f72e3)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174926#issuecomment-3901139344))
- [Revert "[autogradable leaf module] add effect token support (#174122)"](https://github.com/pytorch/pytorch/commit/b8ad7029c11b2472c184e26580b31d9e00648f0d)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174122#issuecomment-3899854178))
- [Revert "Make Dtensor have consistent cache key in compile (#173526)"](https://github.com/pytorch/pytorch/commit/a7542f8a6ed1aeae8cbe7a976b7a1d4b526ac5e5)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173526#issuecomment-3899571127))
- [Revert "Make Dtensor have consistent cache key in compile (#173526)"](https://github.com/pytorch/pytorch/commit/537496eded4954f7b8e555d05806f02b46461782)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173526#issuecomment-3899370361))
- [Revert "[cuDNN] Upgrade cuDNN to 9.19 for 12.8 and 13.0 wheels (#174310)"](https://github.com/pytorch/pytorch/commit/7c963eb9f94aecfc7bd0cc061287ecc215d5f597)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174310#issuecomment-3899331370))
- [Revert "[dynamo] dynamo bytecode debugger (#173859)"](https://github.com/pytorch/pytorch/commit/976d4eea6aba5f9b786bd9f0126211f0be97fb6c)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173859#issuecomment-3898557109))
- [Revert "[dynamo] pause bdb if exception encountered (#174407)"](https://github.com/pytorch/pytorch/commit/b54c7ec400cca4a44804dcaa02a7af42e6f3676a)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173859#issuecomment-3898557109))
- [Revert "[dynamo, bdb] test for empty command (#174500)"](https://github.com/pytorch/pytorch/commit/b10958ba44379c1bbfb17bda14786b7bf0e2ffd3)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173859#issuecomment-3898557109))
- [Revert "[DTensor] Strategy Validation (2/3): partial input creation and validation engine (#174799)"](https://github.com/pytorch/pytorch/commit/5871f7a470e38cf3cd1b176b15db487daeab1e89)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174799#issuecomment-3897674711))
- [Revert "[FSDP2] Remove dynamo tracing support from fully_shard (#174863)"](https://github.com/pytorch/pytorch/commit/c67d667961e3990635a87f405ba662ad7af702a2)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174863#issuecomment-3894832992))
- [Revert "[FSDP2] support per-param mesh (#173509)"](https://github.com/pytorch/pytorch/commit/1e05211983257235904f66aa2b448d6dc130877f)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173509#issuecomment-3893563194))
- [Revert "Add `CALL_INTRINSIC_1 7` and `CALL_INTRINSIC_2 4` (#174450)"](https://github.com/pytorch/pytorch/commit/9d1f7dad00561388208b17198eaba64220933469)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174450#issuecomment-3892709980))
- [Revert "[inductor] overlap scheduling: schedule pre bucketed off path (#170578)"](https://github.com/pytorch/pytorch/commit/7b83926b1888994301f9874b79072da7e39a8482)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170578#issuecomment-3892089847))
- [Revert "[cuDNN] Upgrade cuDNN to 9.19 for 12.8 and 13.0 wheels (#174310)"](https://github.com/pytorch/pytorch/commit/fbdc5871b53ecf77ffd848718803b0d16a76b6c4)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174310#issuecomment-3888830094))
- [Revert "[CUDA][cuBLASLt] set cuBLASLt as a default BLAS backend when available (#174594)"](https://github.com/pytorch/pytorch/commit/f40b3ceba9c8d6c1ea0d39a574b8949d918562f9)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174594#issuecomment-3888498966))
- [Revert "[DTensor] avoid P(sum) propagation on L0-norm (#174617)"](https://github.com/pytorch/pytorch/commit/56a6a6c7510ef763dbc1b7f43c5ac75429416def)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174617#issuecomment-3888444881))
- [Revert "[DTensor] assume uneven sharding by default, redistribute cost hints for unbacked (#168052)"](https://github.com/pytorch/pytorch/commit/0a7acd052f32ea70693ce5a113990dff3632de42)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/168052#issuecomment-3888302358))
- [Revert "[Inductor] use two step algorithm to calculate variance (#170757)"](https://github.com/pytorch/pytorch/commit/9df5f442acf6c3227d654a46a4bcff7d6a099472)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/170757#issuecomment-3881274387))
- [Revert "[c10] Move P2P access logic from ATen to c10 (#174582)"](https://github.com/pytorch/pytorch/commit/947729b8be8d33d45fe613e0f14885a55fbc2dba)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174582#issuecomment-3879648907))
- [Revert "[AOTI] Fix proxy executor ScalarType/MemoryFormat/Layout deserialization (#173562)"](https://github.com/pytorch/pytorch/commit/cd25a783911b9bbf4331413f7153c8e86b983a22)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/173562#issuecomment-3878876184))
- [Revert "[CUDA][cuBLASLt] fix CUBLAS_STATUS_INVALID_VALUE when performing X[.mH]? @ Y[.mH]? (#174479)"](https://github.com/pytorch/pytorch/commit/bd780b31e6f36d1c8ffd98db6e06b349ca8862dc)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/174479#issuecomment-3874136874))
- [Revert "[Inductor XPU GEMM] Step 4/N: Refactor CUDAKernel to CUTLASSKernel. (#160687)"](https://github.com/pytorch/pytorch/commit/58ddb859bac6ae6b8f4f67494c56e5d0208ffb10)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/160687#issuecomment-3871458232))
- [Revert "[Inductor XPU GEMM] Step 4/N: Refactor CUDAKernel to CUTLASSKernel. (#160687)"](https://github.com/pytorch/pytorch/commit/ab106f7e04180861aa4fae7944bbe326133f5df4)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/160687#issuecomment-3869371781))
- [Revert "xpu: add a test to verify all torch xpu libraries are linked on Linux (#169322)"](https://github.com/pytorch/pytorch/commit/8d0087c0713ff2747cdce03e861832d2f784033a)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/169322#issuecomment-3869046562))

### GHFirst (8)

- [Revert "[FSDP2] support per-param mesh (#173509)"](https://github.com/pytorch/pytorch/commit/cb24566270fa80c9bd1b219b9439ee9b3637b60e)
  - failing internal tests see D93334271 ([comment](https://github.com/pytorch/pytorch/pull/173509#issuecomment-3904971577))
- [Revert "Reserve vector capacity in AdvancedIndex constructor (#174782)"](https://github.com/pytorch/pytorch/commit/3c77f2d8d9f3ad8e7d1f477764c773bdd4905500)
  - Weird revert, need to do since this is blocking internal exports of code see T254702875 ([comment](https://github.com/pytorch/pytorch/pull/174782#issuecomment-3898179851))
- [Revert "c10d: convert NanCheck to an op + tests (#174736)"](https://github.com/pytorch/pytorch/commit/31e65e59790dcd0947a34f22c264695745247842)
  - There are internal failures related to this, can you check D92987548 and re-land via co-dev ([comment](https://github.com/pytorch/pytorch/pull/174736#issuecomment-3894124238))
- [Revert "Upgrade pyrefly to 0.52.0 (#174426)"](https://github.com/pytorch/pytorch/commit/03fa06fbc3a1afbb078574de66a0f13b89c5a632)
  - This is causing issues with internal import since it contains a change that touches a reverted file, would recommend to re-land as a co-dev diff to avoid this ([comment](https://github.com/pytorch/pytorch/pull/174426#issuecomment-3891966292))
- [Revert "Add an email to GitHub usernames contributor map (#172785)"](https://github.com/pytorch/pytorch/commit/a35c141fbf69ba498c1490a0a91db18e6e51ca3a)
  - Some of these words are on the banned words list, see D92987485 ([comment](https://github.com/pytorch/pytorch/pull/172785#issuecomment-3891948628))
- [Revert "dynamo torch factory functions apply default device mirroring __torchâ€¦ (#169081)"](https://github.com/pytorch/pytorch/commit/ce7d5fadcb3198d5f98ddfefb27601aee459f29f)
  - This was actually reverted internally @mlazos can you help land this internally via co-dev? ([comment](https://github.com/pytorch/pytorch/pull/169081#issuecomment-3891881571))
- [Revert "[xpu][ut] Generalize device factory functions UT (#174755)"](https://github.com/pytorch/pytorch/commit/da31b80494d1895df3e9271f0694f29c5e2f2bb9)
  - The commit this fixes things for should've been reverted so reverting this first, then will revert that second ([comment](https://github.com/pytorch/pytorch/pull/174755#issuecomment-3891868236))
- [Revert "Make Dtensor have consistent cache key in compile (#173526)"](https://github.com/pytorch/pytorch/commit/d14f1c119e41c5aa11a86a93afd04e319272a7e0)
  - Causing merge conflicts internally, can you re-land as co-dev? ([comment](https://github.com/pytorch/pytorch/pull/173526#issuecomment-3880891041))

### Ignored Signal (1)

- [Revert "[ROCm] Update expected benchmarks for gfx950 (#174157)"](https://github.com/pytorch/pytorch/commit/ed0b1fec7e3b9e3b8d767506696506059b1ad2b0)
  - I missed the lint error ([comment](https://github.com/pytorch/pytorch/pull/174157#issuecomment-3873145668))

### Landrace (2)

- [Revert "[MPS] Fix `abs` complex overflow/underflow (#174346)"](https://github.com/pytorch/pytorch/commit/d1ea1c2a96654612507a02de57f051e9652c1f6a)
  -  started failing on cuda and rocm but only after merge; test/test_sparse_csr.py::TestSparseCSRCUDA::test_sparse_csr_unary_inplace_abs_cuda_complex64 [GH job link](https://github.com/pytorch/pytorch/actions/runs/21922865087/job/63311171484) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/75530cd8b2edef0467862e6ed1c46b2a32376728) ([comment](https://github.com/pytorch/pytorch/pull/174346#issuecomment-3888059118))
- [Revert "Fix XBLOCK' too large. Maximum: 4096. Actual: 8192[Fixed by AI] (#173435)"](https://github.com/pytorch/pytorch/commit/73259427ee8fdf285e2d0f0ef1f8bd32b5e194a4)
  - this PR should have been rebased after ROCm CI changes, prior rocm signal was using gfx942 runners and passing, but post-merge used gfx950 and failed ([comment](https://github.com/pytorch/pytorch/pull/173435#issuecomment-3880546379))

### Not through pytorchbot (1)

- [Back out "Revert D92675476: [c10] Move P2P access logic from ATen to c10" (#174698)](https://github.com/pytorch/pytorch/commit/fb72fee86eb35f5aa62c02ed7dee5ad7471172b6)

### No Signal (12)

- [Revert "Add Python decomposition for quantile/nanquantile to fix torch.export (#174787)"](https://github.com/pytorch/pytorch/commit/cfc49c471ed314a055d60931906edd79b77557eb)
  - Looks like this is causing failures upstream see https://hud.pytorch.org/pytorch/pytorch/commit/4504c3dcee3c02886fae3340a8ee268717c6cb32 ([comment](https://github.com/pytorch/pytorch/pull/174787#issuecomment-3902348911))
- [Revert "[FSDP2] Remove dynamo tracing support from fully_shard (#174863)"](https://github.com/pytorch/pytorch/commit/73fed9f1ad70c7e004cd2bd460cf324c3f79fb1b)
  - some conda build error internal. revert now and land later when i am back from holidy ([comment](https://github.com/pytorch/pytorch/pull/174863#issuecomment-3900533290))
- [Revert "[FSDP2] support per-param mesh (#173509)"](https://github.com/pytorch/pytorch/commit/11c0801c235186f668dc85f44e8087d08cac1a74)
  - some conda build error internal. revert now and land later when i am back from holidy ([comment](https://github.com/pytorch/pytorch/pull/173509#issuecomment-3900155959))
- [Revert "[dynamo, bdb] add l [n] and s [n] commands (#174501)"](https://github.com/pytorch/pytorch/commit/c2567c1f1f4f55b1e43577f4ee91800f2fc7d51c)
  - Not sure what you do here, but this makes a lot of jobs consistently timeout ([comment](https://github.com/pytorch/pytorch/pull/174501#issuecomment-3897916542))
- [Revert "Add CI job to run CPython tests (#174414)"](https://github.com/pytorch/pytorch/commit/cb4dd68ab735da25fc277feb40295aed81906e9b)
  - Looks like this is failing since it's been merged into main, https://hud.pytorch.org/pytorch/pytorch/commit/391682ea327c3ead03c23d254ee67575a24cf2f2 ([comment](https://github.com/pytorch/pytorch/pull/174414#issuecomment-3895433420))
- [Revert "[dynamo] Graph break on triton.set_allocator with nicer message (#174789)"](https://github.com/pytorch/pytorch/commit/a7c4ed2a3e4d56961498cec74604ff17556a6975)
  - Need to revert https://github.com/pytorch/pytorch/pull/174789. See: https://github.com/pytorch/pytorch/pull/174789#issuecomment-3892745701 ([comment](https://github.com/pytorch/pytorch/pull/174789#issuecomment-3892789279))
- [Revert "[dynamo] Skip guards on inspect related variable trackers (#174790)"](https://github.com/pytorch/pytorch/commit/6d4c54c0df3a3a20a4ad8861a9b0870bc406569c)
  - Need to revert https://github.com/pytorch/pytorch/pull/174789. See: https://github.com/pytorch/pytorch/pull/174789#issuecomment-3892745701 ([comment](https://github.com/pytorch/pytorch/pull/174790#issuecomment-3892772273))
- [Revert "[Inductor] Fix combo kernel y grid overflow (#174354)"](https://github.com/pytorch/pytorch/commit/ad0bd5b3d24070914b6f282d8cd9f2e402e7adde)
  - Broken trunk because this runs out of memory ([comment](https://github.com/pytorch/pytorch/pull/174354#issuecomment-3887199933))
- [Revert "[DTensor] forward fix scatter xfails (#174757)"](https://github.com/pytorch/pytorch/commit/478905000bdaec937d5443d969b4e4ea9548dcf0)
  - https://github.com/pytorch/pytorch/pull/173404 was reverted mid-land ([comment](https://github.com/pytorch/pytorch/pull/174757#issuecomment-3886415538))
- [Revert "[dynamic shapes] fix scatter_shape_check DDEs (#173404)"](https://github.com/pytorch/pytorch/commit/ec4f1c7a19d41df4e145fdb83ae3328d16807122)
  - Breaks DTensor ([comment](https://github.com/pytorch/pytorch/pull/173404#issuecomment-3886158621))
- [Revert "[DTensor] Fix argmax/argmin sharding strategy (#173936)"](https://github.com/pytorch/pytorch/commit/01986785a11392ea3fd5061fcdb53dbe383102c0)
  - Newly added test is broken on trunk ([comment](https://github.com/pytorch/pytorch/pull/173936#issuecomment-3881057388))
- [Revert "Upgrade pyrefly to 0.52.0 (#174426)"](https://github.com/pytorch/pytorch/commit/55571b6c22cdfcc2c76c6b4e7ef6a5ad9eac0b56)
  - Did it broke lint by accident ([comment](https://github.com/pytorch/pytorch/pull/174426#issuecomment-3875266554))

### Weird (1)

- [Revert "[DTensor] enable single dim strategy for mm and bmm (#172385)"](https://github.com/pytorch/pytorch/commit/3cbed8e618ac490f4e815ea425c91daedcfe6dc1)
  - broke ROCm, signal was missed but still allowed to land ([comment](https://github.com/pytorch/pytorch/pull/172385#issuecomment-3891557399))
