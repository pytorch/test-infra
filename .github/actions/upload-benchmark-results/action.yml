name: Upload benchmark results

inputs:
  benchmark-results-dir:
    description: 'The path to the directory with all the results in JSON format'
    required: true
  benchmark-name:
    description: 'Manually set the name of the benchmark'
    default: ''
  dry-run:
    default: 'true'
  schema-version:
    default: 'v3'
  github-token:
    default: ''
  venv:
    description: 'Path to virtual environment to activate'
    default: ''
  if-no-files-found:
    description: The desired behavior if no benchmark results are found
    # Available Options:
    #   error: Fail the action with an error message
    #   ignore: Do not output any warnings or errors, the action does not fail
    # Optional. Default is 'ignore'
    default: ignore

runs:
  using: composite
  steps:
    - name: Install dependencies
      shell: bash
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi
        python3 -mpip install boto3==1.35.33 psutil==7.0.0 nvidia-ml-py==13.580.82

    - name: Get device name
      shell: bash
      run: |
        set -eux

        if command -v nvidia-smi; then
          DEVICE_NAME=cuda
          nvidia-smi
        elif command -v rocm-smi; then
          DEVICE_NAME=rocm
          rocm-smi
        elif command -v hl-smi; then
          DEVICE_NAME=hpu
          hl-smi
        else
          arch=$(uname -m)

          case "$arch" in
            aarch64|arm64)
              DEVICE_NAME=arm64-cpu
              ;;
            *)
              DEVICE_NAME=cpu
              ;;
          esac
          lscpu
        fi
        echo "DEVICE_NAME=$DEVICE_NAME" >> $GITHUB_ENV

    - name: Get device type
      shell: bash
      run: |
        set -eux

        if [[ "${DEVICE_NAME}" == "cuda" ]]; then
          DEVICE_TYPE=$(nvidia-smi -i 0 --query-gpu=name --format=csv,noheader | awk '{print $2}')
        elif [[ "${DEVICE_NAME}" == "rocm" ]]; then
          DEVICE_TYPE=$(rocminfo | grep "Marketing Name" | tail -n1 | awk -F':' '{print $2}' | xargs)
        elif [[ "${DEVICE_NAME}" == "hpu" ]]; then
          DEVICE_TYPE="Intel Gaudi3 "$(hl-smi -q | grep "Product Name" | head -n 1 | awk -F ':' '{print $2}' | sed 's/^ *//')
        elif [[ "${DEVICE_NAME}" == "cpu" ]]; then
          DEVICE_TYPE="$(lscpu | grep "Model name" | sed -E 's/.*Model name:[[:space:]]*//; s/Intel\(R\)//g; s/\(R\)//g; s/\(TM\)//g; s/CPU//g; s/Processor//g; s/[[:space:]]+/ /g; s/^ //; s/ $//; s/ /_/g')_$(awk -F: '/Core\(s\) per socket/ {c=$2} /Socket\(s\)/ {s=$2} END {gsub(/ /,"",c); gsub(/ /,"",s); printf "%sc", c*s}' < <(lscpu))"
        elif [[ "${DEVICE_NAME}" == "arm64-cpu" ]]; then
          DEVICE_TYPE=$(lscpu | grep 'Vendor ID' | cut -f 2 -d ":" | awk '{$1=$1}1' | cut -f 2 -d " ")
        fi
        echo "DEVICE_TYPE=$DEVICE_TYPE" >> $GITHUB_ENV

    - name: Check that GITHUB_TOKEN is defined
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      shell: bash
      run: |
        set -eux

        if [[ -z "${GITHUB_TOKEN}" ]]; then
          echo "Missing github-token input"
          exit 1
        fi

    - name: Get workflow job id
      if: ${{ inputs.github-token != '' }}
      id: get-job-id
      uses: pytorch/test-infra/.github/actions/get-workflow-job-id@main
      with:
        github-token: ${{ inputs.github-token }}

    - name: Gather the metadata
      id: gather-metadata
      shell: bash
      env:
        SCHEMA_VERSION: ${{ inputs.schema-version }}
        REPO: ${{ github.repository }}
        HEAD_BRANCH: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.ref || github.ref }}
        HEAD_SHA: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
        WORKFLOW_RUN_ID: ${{ github.run_id }}
        RUN_ATTEMPT: ${{ github.run_attempt }}
        JOB_ID: ${{ inputs.github-token != '' && steps.get-job-id.outputs.job-id || '0' }}
        JOB_NAME: ${{ inputs.github-token != '' && steps.get-job-id.outputs.job-name || '' }}
        BENCHMARK_NAME: ${{ inputs.benchmark-name || '' }}
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi

        python3 "${GITHUB_ACTION_PATH}/../../scripts/benchmarks/gather_metadata.py" \
          --schema-version "${SCHEMA_VERSION}" \
          --repo "${REPO}" \
          --head-branch "${HEAD_BRANCH}" \
          --head-sha "${HEAD_SHA}" \
          --workflow-id "${WORKFLOW_RUN_ID}" \
          --run-attempt "${RUN_ATTEMPT}" \
          --job-id "${JOB_ID}" \
          --job-name "${JOB_NAME}"

    - name: Gather the runner information
      id: gather-runner-info
      shell: bash
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi

        python3 "${GITHUB_ACTION_PATH}/../../scripts/benchmarks/gather_runners_info.py"

    - name: Gather the dependencies information
      id: gather-dependencies
      shell: bash
      run: |
        set -eux

        # TODO (huydhn): Implement this part
        echo "dependencies={}" >> "${GITHUB_OUTPUT}"

    - name: Upload benchmark results
      shell: bash
      env:
        BENCHMARK_RESULTS_DIR: ${{ inputs.benchmark-results-dir }}
        BENCHMARK_NAME: ${{ inputs.benchmark-name || '' }}
        DRY_RUN: ${{ inputs.dry-run }}
        IF_NO_FILES_FOUND: ${{ inputs.if-no-files-found || 'ignore' }}
        # Additional information about the benchmarks
        BENCHMARK_METADATA: ${{ steps.gather-metadata.outputs.metadata }}
        RUNNER_INFO: ${{ steps.gather-runner-info.outputs.runners }}
        DEPENDENCIES: ${{ steps.gather-dependencies.outputs.dependencies }}
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi

        if [[ ! -d "${BENCHMARK_RESULTS_DIR}" ]]; then
          echo "${BENCHMARK_RESULTS_DIR} does not exist, skipping"
          # We don't want the job to fail if the directory doesn't exist
          exit 0
        fi

        if [[ "${DRY_RUN}" == "true" ]]; then
          python3 "${GITHUB_ACTION_PATH}/../../scripts/upload_benchmark_results.py" \
            --benchmark-results-dir "${BENCHMARK_RESULTS_DIR}" \
            --metadata "${BENCHMARK_METADATA}" \
            --runners "${RUNNER_INFO}" \
            --dependencies "${DEPENDENCIES}" \
            --dry-run
        else
          python3 "${GITHUB_ACTION_PATH}/../../scripts/upload_benchmark_results.py" \
            --benchmark-results-dir "${BENCHMARK_RESULTS_DIR}" \
            --metadata "${BENCHMARK_METADATA}" \
            --runners "${RUNNER_INFO}" \
            --dependencies "${DEPENDENCIES}"
        fi
