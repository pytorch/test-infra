name: Upload benchmark results

inputs:
  benchmark-results-dir:
    description: 'The path to the directory with all the results in JSON format'
    required: True
  dry-run:
    default: 'true'
  schema-version:
    default: 'v3'
  github-token:
    default: ''
  venv:
    description: 'Path to virtual environment to activate'
    required: false
    default: ''

runs:
  using: composite
  steps:
    - name: Install dependencies
      shell: bash
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi
        python3 -mpip install boto3==1.35.33 psutil==7.0.0 pynvml==12.0.0

        DEVICE_NAME=""
        DEVICE_TYPE=""

        if command -v nvidia-smi; then
          # NB: I'm using PyTorch here to get the device name, however, it needs to
          # install the correct version of PyTorch manually for now. Any PyTorch
          # version is fine, I just use 2.7.1 to satify PYPIDEP linter
          python3 -mpip install torch==2.7.1
        elif command -v rocminfo; then
          # NB: Installing torch on ROCm runner with pip here causes CI to fail
          # with a memoryview is too large error only on MI300 runners. Is pip
          # version on ROCm runner there too old? As a workaround, let's use the
          # GPU device name coming from rocminfo instead
          DEVICE_NAME=rocm
          DEVICE_TYPE=$(rocminfo | grep "Marketing Name" | tail -n1 | awk -F':' '{print $2}' | xargs)
          ELAINE_TESTING_TYPE=$(rocminfo | grep "Marketing Name")
          echo "GPU(s) found:"
          echo "$ELAINE_TESTING_TYPE"
        fi

        echo "DEVICE_NAME=$DEVICE_NAME" >> $GITHUB_ENV
        echo "DEVICE_TYPE=$DEVICE_TYPE" >> $GITHUB_ENV

    - name: Check that GITHUB_TOKEN is defined
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      shell: bash
      run: |
        set -eux

        if [[ -z "${GITHUB_TOKEN}" ]]; then
          echo "Missing github-token input"
          exit 1
        fi

    - name: Get workflow job id
      if: ${{ inputs.github-token != '' }}
      id: get-job-id
      uses: pytorch/test-infra/.github/actions/get-workflow-job-id@main
      with:
        github-token: ${{ inputs.github-token }}

    - name: Gather the metadata
      id: gather-metadata
      shell: bash
      env:
        SCHEMA_VERSION: ${{ inputs.schema-version }}
        REPO: ${{ github.repository }}
        HEAD_BRANCH: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.ref || github.ref }}
        HEAD_SHA: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
        WORKFLOW_RUN_ID: ${{ github.run_id }}
        RUN_ATTEMPT: ${{ github.run_attempt }}
        JOB_ID: ${{ inputs.github-token != '' && steps.get-job-id.outputs.job-id || '0' }}
        JOB_NAME: ${{ inputs.github-token != '' && steps.get-job-id.outputs.job-name || '' }}
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi

        python3 "${GITHUB_ACTION_PATH}/../../scripts/benchmarks/gather_metadata.py" \
          --schema-version "${SCHEMA_VERSION}" \
          --repo "${REPO}" \
          --head-branch "${HEAD_BRANCH}" \
          --head-sha "${HEAD_SHA}" \
          --workflow-id "${WORKFLOW_RUN_ID}" \
          --run-attempt "${RUN_ATTEMPT}" \
          --job-id "${JOB_ID}" \
          --job-name "${JOB_NAME}"

    - name: Gather the runner information
      id: gather-runner-info
      shell: bash
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi

        python3 "${GITHUB_ACTION_PATH}/../../scripts/benchmarks/gather_runners_info.py"

    - name: Gather the dependencies information
      id: gather-dependencies
      shell: bash
      run: |
        set -eux

        # TODO (huydhn): Implement this part
        echo "dependencies={}" >> "${GITHUB_OUTPUT}"

    - name: Upload benchmark results
      shell: bash
      env:
        BENCHMARK_RESULTS_DIR: ${{ inputs.benchmark-results-dir }}
        DRY_RUN: ${{ inputs.dry-run }}
        # Additional information about the benchmarks
        BENCHMARK_METADATA: ${{ steps.gather-metadata.outputs.metadata }}
        RUNNER_INFO: ${{ steps.gather-runner-info.outputs.runners }}
        DEPENDENCIES: ${{ steps.gather-dependencies.outputs.dependencies }}
      run: |
        set -eux

        if [[ -n "${{ inputs.venv }}" ]]; then
          source "${{ inputs.venv }}"
        fi

        if [[ ! -d "${BENCHMARK_RESULTS_DIR}" ]]; then
          echo "${BENCHMARK_RESULTS_DIR} does not exist, skipping"
          # We don't want the job to fail if the directory doesn't exist
          exit 0
        fi

        if [[ "${DRY_RUN}" == "true" ]]; then
          python3 "${GITHUB_ACTION_PATH}/../../scripts/upload_benchmark_results.py" \
            --benchmark-results-dir "${BENCHMARK_RESULTS_DIR}" \
            --metadata "${BENCHMARK_METADATA}" \
            --runners "${RUNNER_INFO}" \
            --dependencies "${DEPENDENCIES}" \
            --dry-run
        else
          python3 "${GITHUB_ACTION_PATH}/../../scripts/upload_benchmark_results.py" \
            --benchmark-results-dir "${BENCHMARK_RESULTS_DIR}" \
            --metadata "${BENCHMARK_METADATA}" \
            --runners "${RUNNER_INFO}" \
            --dependencies "${DEPENDENCIES}"
        fi
