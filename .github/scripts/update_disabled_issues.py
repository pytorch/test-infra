#!/usr/bin/env python3
"""
Query for the DISABLED and UNSTABLE issues and generate the JSON files that go
in the stats folder of the generated-stats branch.
"""

import json
import os
import time
from datetime import datetime, timezone
from typing import Any, Dict
from urllib.request import Request, urlopen

from gen_historical_disabled_issue_data import format_info


HUD_URL = "https://hud.pytorch.org"


def dump_json(data: Dict[str, Any], filename: str):
    with open(filename, mode="w") as file:
        json.dump(data, file, sort_keys=True, indent=2)


def main() -> None:
    with urlopen(
        Request(
            f"{HUD_URL}/api/flaky-tests/getDisabledTestsAndJobs",
            headers={"Authorization": os.environ["FLAKY_TEST_BOT_KEY"]},
        )
    ) as result:
        if result.status != 200:
            raise RuntimeError(f"Failed to fetch data: {result.status} {result.reason}")

        json_data = json.loads(result.read().decode("utf-8"))

    dump_json(json_data["disabledTests"], "disabled-tests-condensed.json")
    dump_json(json_data["disabledJobs"], "disabled-jobs.json")
    dump_json(json_data["unstableJobs"], "unstable-jobs.json")

    # All the info generated by this can be regenerated if we need to backfill
    # the historical data, so we don't need to keep it in the repo.  The
    # timestamp is going to be sligtly off, but its a small enough difference
    # that I'm willing to live with it.
    with open("for_historical_records.json", mode="w") as file:
        timestamp = time.time()
        day = datetime.fromtimestamp(timestamp, tz=timezone.utc).date().isoformat()
        info = format_info(json_data["disabledTests"], day, int(timestamp))
        for item in info:
            json.dump(item, file)
            file.write("\n")


if __name__ == "__main__":
    main()
