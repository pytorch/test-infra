import libcst as cst
import libcst.matchers as m


from ...common import TorchVisitor, LintViolation


class TorchPrivateSymbolsVisitor(TorchVisitor):
    """
    Find usages of non-public PyTorch symbols.
    """

    ERROR_CODE = "TOR103"
    MESSAGE_START = "Use of non-public symbol"

    # Copied from https://github.com/pytorch/pytorch/blob/main/test/allowlist_for_publicAPI.json
    UNDERSCORE_PUBLIC_APIS = [
        "_adaptive_avg_pool2d",
        "_adaptive_avg_pool3d",
        "_add_batch_dim",
        "_add_relu",
        "_add_relu_",
        "_addmm_activation",
        "_aminmax",
        "_amp_foreach_non_finite_check_and_unscale_",
        "_amp_update_scale_",
        "_assert_async",
        "_batch_norm_impl_index",
        "_cast_Byte",
        "_cast_Char",
        "_cast_Double",
        "_cast_Float",
        "_cast_Half",
        "_cast_Int",
        "_cast_Long",
        "_cast_Short",
        "_choose_qparams_per_tensor",
        "_coalesce",
        "_compute_linear_combination",
        "_conj",
        "_conj_copy",
        "_conj_physical",
        "_convert_indices_from_coo_to_csr",
        "_convert_indices_from_csr_to_coo",
        "_convolution",
        "_convolution_mode",
        "_copy_from",
        "_copy_from_and_resize",
        "_ctc_loss",
        "_cudnn_ctc_loss",
        "_cudnn_init_dropout_state",
        "_cudnn_rnn",
        "_cudnn_rnn_flatten_weight",
        "_cufft_clear_plan_cache",
        "_cufft_get_plan_cache_max_size",
        "_cufft_get_plan_cache_size",
        "_cufft_set_plan_cache_max_size",
        "_cummax_helper",
        "_cummin_helper",
        "_debug_has_internal_overlap",
        "_det_lu_based_helper_backward_helper",
        "_dim_arange",
        "_dirichlet_grad",
        "_disable_functionalization",
        "_efficientzerotensor",
        "_embedding_bag",
        "_embedding_bag_forward_only",
        "_empty_affine_quantized",
        "_empty_per_channel_affine_quantized",
        "_enable_functionalization",
        "_euclidean_dist",
        "_fake_quantize_learnable_per_channel_affine",
        "_fake_quantize_learnable_per_tensor_affine",
        "_fake_quantize_per_tensor_affine_cachemask_tensor_qparams",
        "_fft_c2c",
        "_fft_c2r",
        "_fft_r2c",
        "_foreach_abs",
        "_foreach_abs_",
        "_foreach_acos",
        "_foreach_acos_",
        "_foreach_add",
        "_foreach_add_",
        "_foreach_addcdiv",
        "_foreach_addcdiv_",
        "_foreach_addcmul",
        "_foreach_addcmul_",
        "_foreach_asin",
        "_foreach_asin_",
        "_foreach_atan",
        "_foreach_atan_",
        "_foreach_ceil",
        "_foreach_ceil_",
        "_foreach_cos",
        "_foreach_cos_",
        "_foreach_cosh",
        "_foreach_cosh_",
        "_foreach_div",
        "_foreach_div_",
        "_foreach_erf",
        "_foreach_erf_",
        "_foreach_erfc",
        "_foreach_erfc_",
        "_foreach_exp",
        "_foreach_exp_",
        "_foreach_expm1",
        "_foreach_expm1_",
        "_foreach_floor",
        "_foreach_floor_",
        "_foreach_frac",
        "_foreach_frac_",
        "_foreach_lgamma",
        "_foreach_lgamma_",
        "_foreach_log",
        "_foreach_log10",
        "_foreach_log10_",
        "_foreach_log1p",
        "_foreach_log1p_",
        "_foreach_log2",
        "_foreach_log2_",
        "_foreach_log_",
        "_foreach_maximum",
        "_foreach_minimum",
        "_foreach_mul",
        "_foreach_mul_",
        "_foreach_neg",
        "_foreach_neg_",
        "_foreach_norm",
        "_foreach_reciprocal",
        "_foreach_reciprocal_",
        "_foreach_round",
        "_foreach_round_",
        "_foreach_sigmoid",
        "_foreach_sigmoid_",
        "_foreach_sign",
        "_foreach_sign_",
        "_foreach_sin",
        "_foreach_sin_",
        "_foreach_sinh",
        "_foreach_sinh_",
        "_foreach_sqrt",
        "_foreach_sqrt_",
        "_foreach_sub",
        "_foreach_sub_",
        "_foreach_tan",
        "_foreach_tan_",
        "_foreach_tanh",
        "_foreach_tanh_",
        "_foreach_trunc",
        "_foreach_trunc_",
        "_foreach_zero_",
        "_from_functional_tensor",
        "_fused_dropout",
        "_fused_moving_avg_obs_fq_helper",
        "_fw_primal_copy",
        "_grid_sampler_2d_cpu_fallback",
        "_has_compatible_shallow_copy_type",
        "_histogramdd_bin_edges",
        "_histogramdd_from_bin_cts",
        "_histogramdd_from_bin_tensors",
        "_index_put_impl_",
        "_indices_copy",
        "_is_functional_tensor",
        "_is_zerotensor",
        "_linalg_check_errors",
        "_linalg_qr_helper",
        "_linalg_svd",
        "_linalg_solve_ex",
        "_log_softmax",
        "_log_softmax_backward_data",
        "_logcumsumexp",
        "_lu_with_info",
        "_make_dual",
        "_make_dual_copy",
        "_make_per_channel_quantized_tensor",
        "_make_per_tensor_quantized_tensor",
        "_masked_scale",
        "_masked_softmax",
        "_mkldnn_reshape",
        "_mkldnn_transpose",
        "_mkldnn_transpose_",
        "_neg_view",
        "_neg_view_copy",
        "_nested_from_padded",
        "_nested_from_padded_and_nested_example",
        "_nnpack_available",
        "_nnpack_spatial_convolution",
        "_pack_padded_sequence",
        "_pad_packed_sequence",
        "_pin_memory",
        "_remove_batch_dim",
        "_reshape_alias_copy",
        "_reshape_from_tensor",
        "_rowwise_prune",
        "_sample_dirichlet",
        "_saturate_weight_to_fp16",
        "_shape_as_tensor",
        "_sobol_engine_draw",
        "_sobol_engine_ff_",
        "_sobol_engine_initialize_state_",
        "_sobol_engine_scramble_",
        "_softmax",
        "_softmax_backward_data",
        "_sparse_broadcast_to",
        "_sparse_broadcast_to_copy",
        "_sparse_coo_tensor_unsafe",
        "_sparse_csr_prod",
        "_sparse_csr_sum",
        "_sparse_csr_tensor_unsafe",
        "_sparse_log_softmax_backward_data",
        "_sparse_softmax_backward_data",
        "_sparse_sparse_matmul",
        "_sparse_sum",
        "_stack",
        "_standard_gamma",
        "_standard_gamma_grad",
        "_sync",
        "_test_serialization_subcmul",
        "_to_cpu",
        "_to_functional_tensor",
        "_torch_cuda_cu_linker_symbol_op",
        "_trilinear",
        "_unique",
        "_unique2",
        "_unpack_dual",
        "_use_cudnn_ctc_loss",
        "_use_cudnn_rnn_flatten_weight",
        "_validate_sparse_compressed_tensor_args",
        "_validate_sparse_coo_tensor_args",
        "_validate_sparse_csr_tensor_args",
        "_values_copy",
        "_weight_norm",
        "_weight_norm_interface",
    ]

    def visit_Call(self, node):
        qualified_name = self.get_qualified_name_for_call(node)
        if qualified_name is None:
            return

        parts = qualified_name.split(".")
        if parts[0] != "torch":
            return
        for part in parts:
            if (
                part.startswith("_")
                and not (part.startswith("__") and part.endswith("__"))
                and part not in self.UNDERSCORE_PUBLIC_APIS
            ):
                position_metadata = self.get_metadata(
                    cst.metadata.WhitespaceInclusivePositionProvider, node
                )

                self.violations.append(
                    LintViolation(
                        error_code=self.ERROR_CODE,
                        message=f"{self.MESSAGE_START} {qualified_name}",
                        line=position_metadata.start.line,
                        column=position_metadata.start.column,
                        node=node,
                        replacement=None,
                    )
                )
                break
